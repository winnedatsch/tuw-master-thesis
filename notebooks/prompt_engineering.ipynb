{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/questions/train_sampled_questions_50000.json\") as f:\n",
    "   questions = list(json.load(f).items())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def compute_object_size(scene_graph, object):\n",
    "    image_size = scene_graph[\"width\"] * scene_graph[\"height\"]\n",
    "    object_size = object[\"w\"] * object[\"h\"]\n",
    "    return object_size / image_size\n",
    "\n",
    "class_samples_positive = []\n",
    "attr_samples_positive = []\n",
    "rel_samples_positive = []\n",
    "\n",
    "def object_within_image_bounds(scene_graph, object):\n",
    "    return object[\"x\"] >= 0 and object[\"y\"] >= 0 and object[\"h\"] > 0 and object[\"w\"] > 0 and \\\n",
    "           object[\"x\"] + object[\"w\"] <= scene_graph[\"width\"] and object[\"y\"] + object[\"h\"] <= scene_graph[\"height\"]\n",
    "\n",
    "for qid, question in questions:\n",
    "    for op in question[\"semantic\"]:\n",
    "        operation = op[\"operation\"]\n",
    "        argument = op[\"argument\"].strip()\n",
    "        objects = question[\"sceneGraph\"][\"objects\"]\n",
    "\n",
    "        if operation == \"select\" and argument != \"scene\" and not argument.endswith(\"(-)\"):\n",
    "            matching_objects = [(oid, objects[oid]) for oid in argument.split(\"(\")[1][:-1].split(\",\") if object_within_image_bounds(question[\"sceneGraph\"], objects[oid])] \n",
    "            if len(matching_objects) > 0:\n",
    "                oid, object = random.choice(matching_objects)\n",
    "                object[\"object_id\"] = oid\n",
    "                class_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"class\": argument.split(\"(\")[0].strip(),\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(question[\"sceneGraph\"], object),\n",
    "                        \"y\": True\n",
    "                    })  \n",
    "\n",
    "        elif operation.startswith(\"filter\"):\n",
    "            attr = ' '.join(operation.split(' ')[1:]) if operation != \"filter\" else \"any\"\n",
    "            attr_value = argument[4:-1] if argument.startswith('not(') else argument\n",
    "            objects_with_attr = [(oid, o) for oid, o in objects.items() if attr_value in o[\"attributes\"] and object_within_image_bounds(question[\"sceneGraph\"],o)]\n",
    "            if len(objects_with_attr) > 0 and attr not in [\"hposition\", \"vposition\"]:\n",
    "                oid, object = random.choice(objects_with_attr)\n",
    "                object[\"object_id\"] = oid\n",
    "                attr_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"attr_value\": attr_value,\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(question[\"sceneGraph\"], object),\n",
    "                        \"y\": True\n",
    "                    })\n",
    "\n",
    "        elif operation.startswith(\"verify\"):\n",
    "            attr = ' '.join(operation.split(' ')[1:]) if operation != \"verify\" else \"any\"\n",
    "            attr_value = argument\n",
    "            objects_with_attr = [(oid, o) for oid, o in objects.items() if attr_value in o[\"attributes\"] and object_within_image_bounds(question[\"sceneGraph\"], o)]\n",
    "            if len(objects_with_attr) > 0 and attr not in [\"hposition\", \"vposition\"]:\n",
    "                oid, object = random.choice(objects_with_attr)\n",
    "                object[\"object_id\"] = oid\n",
    "                attr_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"attr_value\": attr_value,\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(question[\"sceneGraph\"], object),\n",
    "                        \"y\": True\n",
    "                    })\n",
    "                \n",
    "        elif operation.startswith(\"choose \") and argument != \"\":\n",
    "            attr = \" \".join(operation.split(\" \")[1:])\n",
    "            attr_value = random.choice([argument.split(\"|\")[0], argument.split(\"|\")[1]])\n",
    "            objects_with_attr = [(oid, o) for oid, o in objects.items() if attr_value in o[\"attributes\"] and object_within_image_bounds(question[\"sceneGraph\"], o)]\n",
    "            if len(objects_with_attr) > 0 and attr not in [\"hposition\", \"vposition\"]:\n",
    "                oid, object = random.choice(objects_with_attr)\n",
    "                object[\"object_id\"] = oid\n",
    "                attr_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"attr_value\": attr_value,\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(question[\"sceneGraph\"], object),\n",
    "                        \"y\": True\n",
    "                    })\n",
    "                \n",
    "        elif operation == \"relate\":\n",
    "            relation_type = argument.split(',')[1]\n",
    "            position = 'subject' if argument.split(',')[2].startswith('s') else 'object'\n",
    "            target_object = argument.split('(')[1][:-1]\n",
    "\n",
    "            if target_object != \"-\":\n",
    "                if position == 'object':\n",
    "                    matching_objects = [(oid, o) for oid, o in objects.items() if any(r[\"object\"] == target_object and r[\"name\"] == relation_type for r in o[\"relations\"]) and object_within_image_bounds(question[\"sceneGraph\"], o)]\n",
    "                    if len(matching_objects) > 0:\n",
    "                        oid0, object0 = random.choice(matching_objects)\n",
    "                        object0[\"object_id\"] = oid0\n",
    "                        object1 = objects[target_object]\n",
    "                        object1[\"object_id\"] = target_object\n",
    "\n",
    "                        rel_samples_positive.append({\n",
    "                                \"question_id\": qid,\n",
    "                                \"question\": question,\n",
    "                                \"image_id\": question[\"imageId\"],\n",
    "                                \"rel\": relation_type,\n",
    "                                \"object0\": object0,\n",
    "                                \"object1\": object1,\n",
    "                                \"object0_size\": compute_object_size(question[\"sceneGraph\"], object0),\n",
    "                                \"object1_size\": compute_object_size(question[\"sceneGraph\"], object1),\n",
    "                                \"y\": True\n",
    "                            })\n",
    "                        \n",
    "                else:\n",
    "                    matching_oids = [r[\"object\"] for r in objects[target_object][\"relations\"] if r[\"name\"] == relation_type]\n",
    "                    matching_objects = [(oid, objects[oid]) for oid in matching_oids if object_within_image_bounds(question[\"sceneGraph\"], objects[oid])]\n",
    "                    if len(matching_objects) > 0:\n",
    "                        object0 = objects[target_object]\n",
    "                        object0[\"object_id\"] = target_object\n",
    "                        oid1, object1 = random.choice(matching_objects)\n",
    "                        object1[\"object_id\"] = oid1\n",
    "\n",
    "                        rel_samples_positive.append({\n",
    "                            \"question_id\": qid,\n",
    "                            \"question\": question,\n",
    "                            \"image_id\": question[\"imageId\"],\n",
    "                            \"rel\": relation_type,\n",
    "                            \"object0\": object0,\n",
    "                            \"object1\": object1,\n",
    "                            \"object0_size\": compute_object_size(question[\"sceneGraph\"], object0),\n",
    "                            \"object1_size\": compute_object_size(question[\"sceneGraph\"], object1),\n",
    "                            \"y\": True\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/metadata/gqa_all_class.json') as f:\n",
    "    categories = json.load(f)\n",
    "class_to_category = {}\n",
    "for category, classes in categories.items():\n",
    "    for c in classes:\n",
    "        if c not in class_to_category:\n",
    "            class_to_category[c] = [category]\n",
    "        else:\n",
    "            class_to_category[c].append(category)\n",
    "\n",
    "class_samples_negative = []\n",
    "for sample in class_samples_positive:\n",
    "    candidate = random.choice(class_samples_positive)\n",
    "    while sample[\"class\"] in [*class_to_category.get(candidate[\"object\"][\"name\"], []), candidate[\"object\"][\"name\"]]:\n",
    "        candidate = random.choice(class_samples_positive)\n",
    "    class_samples_negative.append({\n",
    "        **candidate,\n",
    "        \"class\": sample[\"class\"],\n",
    "        \"y\": False\n",
    "    })\n",
    "class_samples = [*class_samples_positive, *class_samples_negative]\n",
    "random.shuffle(class_samples)\n",
    "\n",
    "attr_samples_negative = []\n",
    "for sample in attr_samples_positive:\n",
    "    candidate = random.choice(attr_samples_positive)\n",
    "    while sample[\"attr_value\"] in candidate[\"object\"][\"attributes\"]:\n",
    "        candidate = random.choice(attr_samples_positive)\n",
    "    \n",
    "    attr_samples_negative.append({\n",
    "        **candidate,\n",
    "        \"attr\": sample[\"attr_value\"],\n",
    "        \"y\": False\n",
    "    })\n",
    "attr_samples = [*attr_samples_positive, *attr_samples_negative]\n",
    "random.shuffle(attr_samples)\n",
    "\n",
    "rel_samples_negative = []\n",
    "for sample in rel_samples_positive:\n",
    "    candidate = random.choice(rel_samples_positive)\n",
    "    while any(r for r in candidate[\"object0\"][\"relations\"] if r[\"name\"] == sample[\"rel\"] and r[\"object\"] == candidate[\"object1\"][\"object_id\"]):\n",
    "        candidate = random.choice(rel_samples_positive)\n",
    "    \n",
    "    rel_samples_negative.append({\n",
    "        **candidate,\n",
    "        \"rel\": sample[\"rel\"],\n",
    "        \"y\": False\n",
    "    })\n",
    "rel_samples = [*rel_samples_positive, *rel_samples_negative]\n",
    "random.shuffle(rel_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPImageProcessor, CLIPTokenizer, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"mps\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "class_samples_flat = [{\n",
    "    \"question_id\": s[\"question_id\"],\n",
    "    \"image_id\": s[\"image_id\"],\n",
    "    \"bbox_x\": s[\"object\"][\"x\"],\n",
    "    \"bbox_y\": s[\"object\"][\"y\"],\n",
    "    \"bbox_w\": s[\"object\"][\"w\"],\n",
    "    \"bbox_h\": s[\"object\"][\"h\"],\n",
    "    \"bbox_size\": s[\"object_size\"],\n",
    "    \"class\": s[\"class\"],\n",
    "    \"y\": s[\"y\"]\n",
    "} for s in class_samples]\n",
    "class_samples_df = pd.DataFrame.from_dict(class_samples_flat)\n",
    "\n",
    "attr_samples_flat = [{\n",
    "    \"question_id\": s[\"question_id\"],\n",
    "    \"image_id\": s[\"image_id\"],\n",
    "    \"bbox_x\": s[\"object\"][\"x\"],\n",
    "    \"bbox_y\": s[\"object\"][\"y\"],\n",
    "    \"bbox_w\": s[\"object\"][\"w\"],\n",
    "    \"bbox_h\": s[\"object\"][\"h\"],\n",
    "    \"bbox_size\": s[\"object_size\"],\n",
    "    \"object_name\": s[\"object\"][\"name\"],\n",
    "    \"attr_value\": s[\"attr_value\"],\n",
    "    \"y\": s[\"y\"]\n",
    "} for s in attr_samples]\n",
    "attr_samples_df = pd.DataFrame.from_dict(attr_samples_flat)\n",
    "\n",
    "rel_samples_flat = []\n",
    "for s in rel_samples:\n",
    "    object0, object1 = s['object0'], s['object1']\n",
    "    joined_bbox = {\n",
    "        \"y\": min(object0['y'], object1['y']),\n",
    "        \"x\": min(object0['x'], object1['x']),\n",
    "        \"h\": max(object0['y'] + object0['h'], object1['y'] + object1['h']) - min(object0['y'], object1['y']),\n",
    "        \"w\": max(object0['x'] + object0['w'], object1['x'] + object1['w']) - min(object0['x'], object1['x']),\n",
    "    }\n",
    "    rel_samples_flat.append({\n",
    "        \"question_id\": s[\"question_id\"],\n",
    "        \"image_id\": s[\"image_id\"],\n",
    "        \"bbox_x\": joined_bbox[\"x\"],\n",
    "        \"bbox_y\": joined_bbox[\"y\"],\n",
    "        \"bbox_w\": joined_bbox[\"w\"],\n",
    "        \"bbox_h\": joined_bbox[\"h\"],\n",
    "        \"bbox_size\": compute_object_size(s[\"question\"][\"sceneGraph\"], joined_bbox),\n",
    "        \"object0_name\": s[\"object0\"][\"name\"],\n",
    "        \"object1_name\": s[\"object1\"][\"name\"],\n",
    "        \"rel\": s[\"rel\"],\n",
    "        \"y\": s[\"y\"]\n",
    "    })\n",
    "rel_samples_df = pd.DataFrame.from_dict(rel_samples_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.transforms.functional import crop, resize, pad\n",
    "from math import tanh\n",
    "\n",
    "def scaling(x, ceiling=3):\n",
    "    return (1 - tanh(x * 2)) * ceiling\n",
    "\n",
    "def get_scaled_bbox(entry, img_height, img_width, padding_scale_ceiling=0.5):\n",
    "    padding_w = scaling(entry[\"bbox_w\"] / img_width, padding_scale_ceiling) * entry[\"bbox_w\"]\n",
    "    padding_h = scaling(entry[\"bbox_h\"] / img_height, padding_scale_ceiling) * entry[\"bbox_h\"]\n",
    "\n",
    "    return (\n",
    "        int(max(entry[\"bbox_y\"] - padding_h, 0)),\n",
    "        int(max(entry[\"bbox_x\"] - padding_w, 0)),\n",
    "        int(min(entry[\"bbox_h\"]+2*padding_h, img_height-max(entry[\"bbox_y\"] - padding_h, 0))),\n",
    "        int(min(entry[\"bbox_w\"]+2*padding_w, img_width-max(entry[\"bbox_x\"] - padding_w, 0)))\n",
    "    )\n",
    "\n",
    "class PromptDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, prompt_transform, img_size=224):\n",
    "        self.df = df \n",
    "        self.prompt_transform = prompt_transform\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.df.iloc[idx]\n",
    "        image = read_image(f\"../data/images/{entry['image_id']}.jpg\", ImageReadMode.RGB)\n",
    "\n",
    "        # crop bounding box\n",
    "        y,x,h,w = get_scaled_bbox(entry, image.shape[1], image.shape[2])\n",
    "        orig_image = crop(image, entry[\"bbox_y\"], entry[\"bbox_x\"], entry[\"bbox_h\"], entry[\"bbox_w\"])\n",
    "        image = crop(image, y, x, h, w)\n",
    "\n",
    "        # resize and scale (maintain aspect ratio)\n",
    "        if entry[\"bbox_h\"] > entry[\"bbox_w\"]:\n",
    "            resize_dimensions = (self.img_size, 2*round((self.img_size*entry[\"bbox_w\"]/entry[\"bbox_h\"])/2)) \n",
    "        else:\n",
    "            resize_dimensions = (2*round((self.img_size*entry[\"bbox_h\"]/entry[\"bbox_w\"])/2), self.img_size)\n",
    "        image = resize(image, resize_dimensions)\n",
    "        orig_image = resize(orig_image, (self.img_size, self.img_size))\n",
    "\n",
    "        # pad the image to square dimensions\n",
    "        image = pad(image, ((self.img_size - resize_dimensions[1])//2, (self.img_size - resize_dimensions[0])//2))\n",
    "\n",
    "        return (image, self.prompt_transform(entry), entry['y'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "class_dataset = PromptDataset(class_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a photo of a {e['class']}\", \n",
    "    \"a photo of an object\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Accuracy: 85.02111053466797%\n"
     ]
    }
   ],
   "source": [
    "class_dataloader = DataLoader(class_dataset, batch_size=batch_size, shuffle=True)\n",
    "class_correct = 0\n",
    "\n",
    "for batch in class_dataloader:\n",
    "    image_inputs = image_processor(batch[0], return_tensors=\"pt\", do_resize=False, do_center_crop=False).to(\"mps\")\n",
    "    text_inputs = tokenizer([*batch[1][0], *batch[1][1]], return_tensors=\"pt\", padding=True).to(\"mps\")\n",
    "    num_items = batch[0].shape[0]\n",
    "\n",
    "    result = model(**image_inputs, **text_inputs)\n",
    "    scores = torch.stack([torch.diagonal(result[\"logits_per_image\"][:,:num_items]), torch.diagonal(result[\"logits_per_image\"][:,num_items:])])\n",
    "    probs = torch.nn.functional.softmax(scores, dim=0)\n",
    "    class_correct += sum((probs[0, :] > probs[1, :]) == batch[2].to(\"mps\"))\n",
    "\n",
    "print(f\"Class Accuracy: {class_correct/class_dataset.__len__()*100}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_dataset = PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a photo of a {e['attr_value']} object\", \n",
    "    f\"a photo of a normal object\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes Accuracy: 50.086944580078125%\n"
     ]
    }
   ],
   "source": [
    "attr_dataloader = DataLoader(attr_dataset, batch_size=batch_size, shuffle=True)\n",
    "attr_correct = 0\n",
    "\n",
    "for batch in attr_dataloader:\n",
    "    image_inputs = image_processor(batch[0], return_tensors=\"pt\", do_resize=False, do_center_crop=False).to(\"mps\")\n",
    "    text_inputs = tokenizer([*batch[1][0], *batch[1][1]], return_tensors=\"pt\", padding=True).to(\"mps\")\n",
    "    num_items = batch[0].shape[0]\n",
    "    \n",
    "    result = model(**image_inputs, **text_inputs)\n",
    "    scores = torch.stack([torch.diagonal(result[\"logits_per_image\"][:,:num_items]), torch.diagonal(result[\"logits_per_image\"][:,num_items:])])\n",
    "    probs = torch.nn.functional.softmax(scores, dim=0)\n",
    "    attr_correct += sum((probs[0, :] > probs[1, :]) == batch[2].to(\"mps\"))\n",
    "\n",
    "print(f\"Attributes Accuracy: {attr_correct/attr_dataset.__len__()*100}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_dataset = PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a photo of a {e['object0_name']} {e['rel']} a {e['object1_name']}\", \n",
    "    f\"a photo of a {e['object0_name']} and a {e['object1_name']}\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/Git/tuw-master-thesis/.venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations Accuracy: 55.75797653198242%\n"
     ]
    }
   ],
   "source": [
    "rel_dataloader = DataLoader(rel_dataset, batch_size=batch_size, shuffle=True)\n",
    "rel_correct = 0\n",
    "\n",
    "for batch in rel_dataloader:\n",
    "    image_inputs = image_processor(batch[0], return_tensors=\"pt\", do_resize=False, do_center_crop=False).to(\"mps\")\n",
    "    text_inputs = tokenizer([*batch[1][0], *batch[1][1]], return_tensors=\"pt\", padding=True).to(\"mps\")\n",
    "\n",
    "    num_items = batch[0].shape[0]\n",
    "    result = model(**image_inputs, **text_inputs)\n",
    "    scores = torch.stack([torch.diagonal(result[\"logits_per_image\"][:,:num_items]), torch.diagonal(result[\"logits_per_image\"][:,num_items:])])\n",
    "    probs = torch.nn.functional.softmax(scores, dim=0)\n",
    "    rel_correct += sum((probs[0, :] > probs[1, :]) == batch[2].to(\"mps\"))\n",
    "\n",
    "print(f\"Relations Accuracy: {rel_correct/rel_dataset.__len__()*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35ab78d29ed88d957c44548976856795b78553768444957779fbd79cbca61a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
