{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# sys path hack to allow importing the encoding functions and other modules\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "sys.path.insert(0, os.path.abspath('../externals'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/questions/val_sampled_questions_50000.json\") as f:\n",
    "   questions = list(json.load(f).items())\n",
    "\n",
    "with open(\"../data/sceneGraphs/val_sceneGraphs.json\") as f:\n",
    "    scene_graphs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def compute_object_size(scene_graph, object):\n",
    "    image_size = scene_graph[\"width\"] * scene_graph[\"height\"]\n",
    "    object_size = object[\"w\"] * object[\"h\"]\n",
    "    return object_size / image_size\n",
    "\n",
    "class_samples_positive = []\n",
    "attr_samples_positive = []\n",
    "rel_samples_positive = []\n",
    "\n",
    "def object_within_image_bounds(scene_graph, object):\n",
    "    return object[\"x\"] >= 0 and object[\"y\"] >= 0 and object[\"h\"] > 0 and object[\"w\"] > 0 and \\\n",
    "           object[\"x\"] + object[\"w\"] <= scene_graph[\"width\"] and object[\"y\"] + object[\"h\"] <= scene_graph[\"height\"]\n",
    "\n",
    "for qid, question in questions:\n",
    "    scene_graph = scene_graphs[question[\"imageId\"]]\n",
    "    for op in question[\"semantic\"]:\n",
    "        operation = op[\"operation\"]\n",
    "        argument = op[\"argument\"].strip()\n",
    "        objects = scene_graph[\"objects\"]\n",
    "\n",
    "        if operation == \"select\" and argument != \"scene\" and not argument.endswith(\"(-)\"):\n",
    "            matching_objects = [(oid, objects[oid]) for oid in argument.split(\"(\")[1][:-1].split(\",\") if object_within_image_bounds(scene_graph, objects[oid])] \n",
    "            if len(matching_objects) > 0:\n",
    "                oid, object = random.choice(matching_objects)\n",
    "                object[\"object_id\"] = oid\n",
    "                class_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"class\": argument.split(\"(\")[0].strip(),\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(scene_graph, object),\n",
    "                        \"y\": True\n",
    "                    })  \n",
    "\n",
    "        elif operation.startswith(\"filter\"):\n",
    "            attr = ' '.join(operation.split(' ')[1:]) if operation != \"filter\" else \"any\"\n",
    "            attr_value = argument[4:-1] if argument.startswith('not(') else argument\n",
    "            objects_with_attr = [(oid, o) for oid, o in objects.items() if attr_value in o[\"attributes\"] and object_within_image_bounds(scene_graph,o)]\n",
    "            if len(objects_with_attr) > 0 and attr not in [\"hposition\", \"vposition\"]:\n",
    "                oid, object = random.choice(objects_with_attr)\n",
    "                object[\"object_id\"] = oid\n",
    "                attr_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"attr_value\": attr_value,\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(scene_graph, object),\n",
    "                        \"y\": True\n",
    "                    })\n",
    "\n",
    "        elif operation.startswith(\"verify\"):\n",
    "            attr = ' '.join(operation.split(' ')[1:]) if operation != \"verify\" else \"any\"\n",
    "            attr_value = argument\n",
    "            objects_with_attr = [(oid, o) for oid, o in objects.items() if attr_value in o[\"attributes\"] and object_within_image_bounds(scene_graph, o)]\n",
    "            if len(objects_with_attr) > 0 and attr not in [\"hposition\", \"vposition\"]:\n",
    "                oid, object = random.choice(objects_with_attr)\n",
    "                object[\"object_id\"] = oid\n",
    "                attr_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"attr_value\": attr_value,\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(scene_graph, object),\n",
    "                        \"y\": True\n",
    "                    })\n",
    "                \n",
    "        elif operation.startswith(\"choose \") and argument != \"\":\n",
    "            attr = \" \".join(operation.split(\" \")[1:])\n",
    "            attr_value = random.choice([argument.split(\"|\")[0], argument.split(\"|\")[1]])\n",
    "            objects_with_attr = [(oid, o) for oid, o in objects.items() if attr_value in o[\"attributes\"] and object_within_image_bounds(scene_graph, o)]\n",
    "            if len(objects_with_attr) > 0 and attr not in [\"hposition\", \"vposition\"]:\n",
    "                oid, object = random.choice(objects_with_attr)\n",
    "                object[\"object_id\"] = oid\n",
    "                attr_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"attr_value\": attr_value,\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(scene_graph, object),\n",
    "                        \"y\": True\n",
    "                    })\n",
    "                \n",
    "        elif operation == \"relate\":\n",
    "            relation_type = argument.split(',')[1]\n",
    "            position = 'subject' if argument.split(',')[2].startswith('s') else 'object'\n",
    "            target_object = argument.split('(')[1][:-1]\n",
    "\n",
    "            if target_object != \"-\":\n",
    "                if position == 'object':\n",
    "                    matching_objects = [(oid, o) for oid, o in objects.items() if any(r[\"object\"] == target_object and r[\"name\"] == relation_type for r in o[\"relations\"]) and object_within_image_bounds(scene_graph, o)]\n",
    "                    if len(matching_objects) > 0:\n",
    "                        oid0, object0 = random.choice(matching_objects)\n",
    "                        object0[\"object_id\"] = oid0\n",
    "                        object1 = objects[target_object]\n",
    "                        object1[\"object_id\"] = target_object\n",
    "\n",
    "                        rel_samples_positive.append({\n",
    "                                \"question_id\": qid,\n",
    "                                \"question\": question,\n",
    "                                \"image_id\": question[\"imageId\"],\n",
    "                                \"rel\": relation_type,\n",
    "                                \"object0\": object0,\n",
    "                                \"object1\": object1,\n",
    "                                \"object0_size\": compute_object_size(scene_graph, object0),\n",
    "                                \"object1_size\": compute_object_size(scene_graph, object1),\n",
    "                                \"y\": True\n",
    "                            })\n",
    "                        \n",
    "                else:\n",
    "                    matching_oids = [r[\"object\"] for r in objects[target_object][\"relations\"] if r[\"name\"] == relation_type]\n",
    "                    matching_objects = [(oid, objects[oid]) for oid in matching_oids if object_within_image_bounds(scene_graph, objects[oid])]\n",
    "                    if len(matching_objects) > 0:\n",
    "                        object0 = objects[target_object]\n",
    "                        object0[\"object_id\"] = target_object\n",
    "                        oid1, object1 = random.choice(matching_objects)\n",
    "                        object1[\"object_id\"] = oid1\n",
    "\n",
    "                        rel_samples_positive.append({\n",
    "                            \"question_id\": qid,\n",
    "                            \"question\": question,\n",
    "                            \"image_id\": question[\"imageId\"],\n",
    "                            \"rel\": relation_type,\n",
    "                            \"object0\": object0,\n",
    "                            \"object1\": object1,\n",
    "                            \"object0_size\": compute_object_size(scene_graph, object0),\n",
    "                            \"object1_size\": compute_object_size(scene_graph, object1),\n",
    "                            \"y\": True\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/metadata/gqa_all_class.json') as f:\n",
    "    categories = json.load(f)\n",
    "class_to_category = {}\n",
    "for category, classes in categories.items():\n",
    "    for c in classes:\n",
    "        if c not in class_to_category:\n",
    "            class_to_category[c] = [category]\n",
    "        else:\n",
    "            class_to_category[c].append(category)\n",
    "\n",
    "class_samples_negative = []\n",
    "for sample in class_samples_positive:\n",
    "    candidate = random.choice(class_samples_positive)\n",
    "    while sample[\"class\"] in [*class_to_category.get(candidate[\"object\"][\"name\"], []), candidate[\"object\"][\"name\"]]:\n",
    "        candidate = random.choice(class_samples_positive)\n",
    "    class_samples_negative.append({\n",
    "        **candidate,\n",
    "        \"class\": sample[\"class\"],\n",
    "        \"y\": False\n",
    "    })\n",
    "class_samples = [*class_samples_positive, *class_samples_negative]\n",
    "random.shuffle(class_samples)\n",
    "\n",
    "attr_samples_negative = []\n",
    "for sample in attr_samples_positive:\n",
    "    candidate = random.choice(attr_samples_positive)\n",
    "    while sample[\"attr_value\"] in candidate[\"object\"][\"attributes\"]:\n",
    "        candidate = random.choice(attr_samples_positive)\n",
    "    \n",
    "    attr_samples_negative.append({\n",
    "        **candidate,\n",
    "        \"attr_value\": sample[\"attr_value\"],\n",
    "        \"y\": False\n",
    "    })\n",
    "attr_samples = [*attr_samples_positive, *attr_samples_negative]\n",
    "random.shuffle(attr_samples)\n",
    "\n",
    "rel_samples_negative = []\n",
    "for sample in rel_samples_positive:\n",
    "    candidate = random.choice(rel_samples_positive)\n",
    "    while any(r for r in candidate[\"object0\"][\"relations\"] if r[\"name\"] == sample[\"rel\"] and r[\"object\"] == candidate[\"object1\"][\"object_id\"]):\n",
    "        candidate = random.choice(rel_samples_positive)\n",
    "    \n",
    "    rel_samples_negative.append({\n",
    "        **candidate,\n",
    "        \"rel\": sample[\"rel\"],\n",
    "        \"y\": False\n",
    "    })\n",
    "rel_samples = [*rel_samples_positive, *rel_samples_negative]\n",
    "random.shuffle(rel_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 18:44:45.627671: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-29 18:44:45.650665: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-29 18:44:45.977054: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "class_samples_flat = [{\n",
    "    \"question_id\": s[\"question_id\"],\n",
    "    \"image_id\": s[\"image_id\"],\n",
    "    \"bbox_x\": s[\"object\"][\"x\"],\n",
    "    \"bbox_y\": s[\"object\"][\"y\"],\n",
    "    \"bbox_w\": s[\"object\"][\"w\"],\n",
    "    \"bbox_h\": s[\"object\"][\"h\"],\n",
    "    \"bbox_size\": s[\"object_size\"],\n",
    "    \"class\": s[\"class\"],\n",
    "    \"y\": s[\"y\"]\n",
    "} for s in class_samples]\n",
    "class_samples_df = pd.DataFrame.from_dict(class_samples_flat)\n",
    "\n",
    "attr_samples_flat = [{\n",
    "    \"question_id\": s[\"question_id\"],\n",
    "    \"image_id\": s[\"image_id\"],\n",
    "    \"bbox_x\": s[\"object\"][\"x\"],\n",
    "    \"bbox_y\": s[\"object\"][\"y\"],\n",
    "    \"bbox_w\": s[\"object\"][\"w\"],\n",
    "    \"bbox_h\": s[\"object\"][\"h\"],\n",
    "    \"bbox_size\": s[\"object_size\"],\n",
    "    \"object_name\": s[\"object\"][\"name\"],\n",
    "    \"attr_value\": s[\"attr_value\"],\n",
    "    \"y\": s[\"y\"]\n",
    "} for s in attr_samples]\n",
    "attr_samples_df = pd.DataFrame.from_dict(attr_samples_flat)\n",
    "\n",
    "rel_samples_flat = []\n",
    "for s in rel_samples:\n",
    "    object0, object1 = s['object0'], s['object1']\n",
    "    joined_bbox = {\n",
    "        \"y\": min(object0['y'], object1['y']),\n",
    "        \"x\": min(object0['x'], object1['x']),\n",
    "        \"h\": max(object0['y'] + object0['h'], object1['y'] + object1['h']) - min(object0['y'], object1['y']),\n",
    "        \"w\": max(object0['x'] + object0['w'], object1['x'] + object1['w']) - min(object0['x'], object1['x']),\n",
    "    }\n",
    "    rel_samples_flat.append({\n",
    "        \"question_id\": s[\"question_id\"],\n",
    "        \"image_id\": s[\"image_id\"],\n",
    "        \"bbox_x\": joined_bbox[\"x\"],\n",
    "        \"bbox_y\": joined_bbox[\"y\"],\n",
    "        \"bbox_w\": joined_bbox[\"w\"],\n",
    "        \"bbox_h\": joined_bbox[\"h\"],\n",
    "        \"bbox_size\": compute_object_size(scene_graphs[s[\"question\"][\"imageId\"]], joined_bbox),\n",
    "        \"object0_name\": s[\"object0\"][\"name\"],\n",
    "        \"object1_name\": s[\"object1\"][\"name\"],\n",
    "        \"rel\": s[\"rel\"],\n",
    "        \"y\": s[\"y\"]\n",
    "    })\n",
    "rel_samples_df = pd.DataFrame.from_dict(rel_samples_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_samples_df.to_pickle(\"../data/finetuning/val_rel_samples_50k.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    gpu = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"Warning: no GPU detected, falling back to CPU\")\n",
    "    gpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/jhadl/.conda/envs/jhadl_tf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Clean old model from cache\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from model.clip_model import CLIPModel\n",
    "model = CLIPModel(gpu)\n",
    "\n",
    "# from model.xvlm_itr_coco_model import XVLMModel\n",
    "# model = XVLMModel(gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target vs. Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "class_dataset = PromptDataset(class_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a bad photo of a {e['class']}\", \n",
    "    \"a bad photo of an object\"\n",
    "], img_size=model.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.107%\n",
      "Precision: 0.938\n",
      "Recall: 0.859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': tensor(90.1065, device='cuda:0'),\n",
       " 'precision': tensor(0.9378, device='cuda:0'),\n",
       " 'recall': tensor(0.8591, device='cuda:0')}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluator import Evaluator\n",
    "\n",
    "class_evaluator = Evaluator(model, class_dataset, batch_size=8)\n",
    "class_evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "attr_dataset = PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a bad photo of a {e['attr_value']} {e['object_name']}\", \n",
    "    f\"a bad photo of a {e['object_name']}\"\n",
    "], img_size=model.img_size, mode=\"pad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.051%\n",
      "Precision: 0.721\n",
      "Recall: 0.556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': tensor(67.0513, device='cuda:0'),\n",
       " 'precision': tensor(0.7214, device='cuda:0'),\n",
       " 'recall': tensor(0.5556, device='cuda:0')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluator import Evaluator\n",
    "\n",
    "attr_evaluator = Evaluator(model, attr_dataset, batch_size=64)\n",
    "attr_evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "rel_dataset = PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a bad photo of a {e['object0_name']} {e['rel']} a {e['object1_name']}\", \n",
    "    f\"a bad photo of a {e['object0_name']} and a {e['object1_name']}\"\n",
    "], img_size=model.img_size, mode=\"scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.302%\n",
      "Precision: 0.574\n",
      "Recall: 0.722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': tensor(59.3022, device='cuda:0'),\n",
       " 'precision': tensor(0.5740, device='cuda:0'),\n",
       " 'recall': tensor(0.7218, device='cuda:0')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluator import Evaluator\n",
    "\n",
    "rel_evaluator = Evaluator(model, rel_dataset, batch_size=64)\n",
    "rel_evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target vs. Contrastive Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/metadata/gqa_all_class.json\") as f:\n",
    "    classes = json.load(f)\n",
    "    \n",
    "with open(\"../data/metadata/gqa_all_attribute.json\") as f:\n",
    "    attributes = json.load(f)\n",
    "    \n",
    "with open(\"../data/metadata/gqa_relation.json\") as f:\n",
    "    relations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "classes_sample = random.sample([item.replace('_', ' ') for items in classes.values() for item in items], 100)\n",
    "attributes_sample = random.sample([item.replace('_', ' ') for items in attributes.values() for item in items], 100)\n",
    "rels_sample = random.sample([item.replace('_', ' ') for item in relations], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_prompts = [f\"a bad photo of a {clazz}\" for clazz in classes_sample]\n",
    "attr_prompts = [f\"a bad photo of a {attr} object\" for attr in attributes_sample]\n",
    "rel_prompts = [f\"a bad photo of an object {rel} an object\" for rel in rels_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "class_dataset = PromptDataset(class_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a bad photo of a {e['class']}\", \n",
    "    \"a bad photo of an object\"\n",
    "], img_size=model.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classes_prompts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mevaluator\u001b[39;00m \u001b[39mimport\u001b[39;00m ContrastiveEvaluator\n\u001b[0;32m----> 3\u001b[0m class_evaluator \u001b[39m=\u001b[39m ContrastiveEvaluator(model, class_dataset, classes_prompts, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m)\n\u001b[1;32m      4\u001b[0m class_evaluator\u001b[39m.\u001b[39mevaluate()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classes_prompts' is not defined"
     ]
    }
   ],
   "source": [
    "from evaluator import ContrastiveEvaluator\n",
    "\n",
    "class_evaluator = ContrastiveEvaluator(model, class_dataset, classes_prompts, batch_size=64)\n",
    "class_evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "attr_dataset = PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a bad photo of a {e['attr_value']} object\", \n",
    "    \"a bad photo of an regular object\"\n",
    "], img_size=model.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.668%\n",
      "Precision: 0.567\n",
      "Recall: 0.564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 56.66811468288445,\n",
       " 'precision': 0.5670110593713621,\n",
       " 'recall': 0.5642195192586157}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluator import ContrastiveEvaluator\n",
    "\n",
    "attr_evaluator = ContrastiveEvaluator(model, attr_dataset, attr_prompts, batch_size=64)\n",
    "attr_evaluator.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "35ab78d29ed88d957c44548976856795b78553768444957779fbd79cbca61a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
