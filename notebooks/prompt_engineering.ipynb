{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# sys path hack to allow importing the encoding functions and other modules\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "sys.path.insert(0, os.path.abspath('../externals'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/questions/val_sampled_questions_50000.json\") as f:\n",
    "   questions = list(json.load(f).items())\n",
    "\n",
    "with open(\"../data/sceneGraphs/val_sceneGraphs.json\") as f:\n",
    "    scene_graphs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def compute_object_size(scene_graph, object):\n",
    "    image_size = scene_graph[\"width\"] * scene_graph[\"height\"]\n",
    "    object_size = object[\"w\"] * object[\"h\"]\n",
    "    return object_size / image_size\n",
    "\n",
    "class_samples_positive = []\n",
    "attr_samples_positive = []\n",
    "rel_samples_positive = []\n",
    "\n",
    "def object_within_image_bounds(scene_graph, object):\n",
    "    return object[\"x\"] >= 0 and object[\"y\"] >= 0 and object[\"h\"] > 0 and object[\"w\"] > 0 and \\\n",
    "           object[\"x\"] + object[\"w\"] <= scene_graph[\"width\"] and object[\"y\"] + object[\"h\"] <= scene_graph[\"height\"]\n",
    "\n",
    "for qid, question in questions:\n",
    "    scene_graph = scene_graphs[question[\"imageId\"]]\n",
    "    for op in question[\"semantic\"]:\n",
    "        operation = op[\"operation\"]\n",
    "        argument = op[\"argument\"].strip()\n",
    "        objects = scene_graph[\"objects\"]\n",
    "\n",
    "        if operation == \"select\" and argument != \"scene\" and not argument.endswith(\"(-)\"):\n",
    "            matching_objects = [(oid, objects[oid]) for oid in argument.split(\"(\")[1][:-1].split(\",\") if object_within_image_bounds(scene_graph, objects[oid])] \n",
    "            if len(matching_objects) > 0:\n",
    "                oid, object = random.choice(matching_objects)\n",
    "                object[\"object_id\"] = oid\n",
    "                class_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"class\": argument.split(\"(\")[0].strip(),\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(scene_graph, object),\n",
    "                        \"y\": True\n",
    "                    })  \n",
    "\n",
    "        elif operation.startswith(\"filter\"):\n",
    "            attr = ' '.join(operation.split(' ')[1:]) if operation != \"filter\" else \"any\"\n",
    "            attr_value = argument[4:-1] if argument.startswith('not(') else argument\n",
    "            objects_with_attr = [(oid, o) for oid, o in objects.items() if attr_value in o[\"attributes\"] and object_within_image_bounds(scene_graph,o)]\n",
    "            if len(objects_with_attr) > 0 and attr not in [\"hposition\", \"vposition\"]:\n",
    "                oid, object = random.choice(objects_with_attr)\n",
    "                object[\"object_id\"] = oid\n",
    "                attr_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"attr_value\": attr_value,\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(scene_graph, object),\n",
    "                        \"y\": True\n",
    "                    })\n",
    "\n",
    "        elif operation.startswith(\"verify\"):\n",
    "            attr = ' '.join(operation.split(' ')[1:]) if operation != \"verify\" else \"any\"\n",
    "            attr_value = argument\n",
    "            objects_with_attr = [(oid, o) for oid, o in objects.items() if attr_value in o[\"attributes\"] and object_within_image_bounds(scene_graph, o)]\n",
    "            if len(objects_with_attr) > 0 and attr not in [\"hposition\", \"vposition\"]:\n",
    "                oid, object = random.choice(objects_with_attr)\n",
    "                object[\"object_id\"] = oid\n",
    "                attr_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"attr_value\": attr_value,\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(scene_graph, object),\n",
    "                        \"y\": True\n",
    "                    })\n",
    "                \n",
    "        elif operation.startswith(\"choose \") and argument != \"\":\n",
    "            attr = \" \".join(operation.split(\" \")[1:])\n",
    "            attr_value = random.choice([argument.split(\"|\")[0], argument.split(\"|\")[1]])\n",
    "            objects_with_attr = [(oid, o) for oid, o in objects.items() if attr_value in o[\"attributes\"] and object_within_image_bounds(scene_graph, o)]\n",
    "            if len(objects_with_attr) > 0 and attr not in [\"hposition\", \"vposition\"]:\n",
    "                oid, object = random.choice(objects_with_attr)\n",
    "                object[\"object_id\"] = oid\n",
    "                attr_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"attr_value\": attr_value,\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(scene_graph, object),\n",
    "                        \"y\": True\n",
    "                    })\n",
    "                \n",
    "        elif operation == \"relate\":\n",
    "            relation_type = argument.split(',')[1]\n",
    "            position = 'subject' if argument.split(',')[2].startswith('s') else 'object'\n",
    "            target_object = argument.split('(')[1][:-1]\n",
    "\n",
    "            if target_object != \"-\":\n",
    "                if position == 'object':\n",
    "                    matching_objects = [(oid, o) for oid, o in objects.items() if any(r[\"object\"] == target_object and r[\"name\"] == relation_type for r in o[\"relations\"]) and object_within_image_bounds(scene_graph, o)]\n",
    "                    if len(matching_objects) > 0:\n",
    "                        oid0, object0 = random.choice(matching_objects)\n",
    "                        object0[\"object_id\"] = oid0\n",
    "                        object1 = objects[target_object]\n",
    "                        object1[\"object_id\"] = target_object\n",
    "\n",
    "                        rel_samples_positive.append({\n",
    "                                \"question_id\": qid,\n",
    "                                \"question\": question,\n",
    "                                \"image_id\": question[\"imageId\"],\n",
    "                                \"rel\": relation_type,\n",
    "                                \"object0\": object0,\n",
    "                                \"object1\": object1,\n",
    "                                \"object0_size\": compute_object_size(scene_graph, object0),\n",
    "                                \"object1_size\": compute_object_size(scene_graph, object1),\n",
    "                                \"y\": True\n",
    "                            })\n",
    "                        \n",
    "                else:\n",
    "                    matching_oids = [r[\"object\"] for r in objects[target_object][\"relations\"] if r[\"name\"] == relation_type]\n",
    "                    matching_objects = [(oid, objects[oid]) for oid in matching_oids if object_within_image_bounds(scene_graph, objects[oid])]\n",
    "                    if len(matching_objects) > 0:\n",
    "                        object0 = objects[target_object]\n",
    "                        object0[\"object_id\"] = target_object\n",
    "                        oid1, object1 = random.choice(matching_objects)\n",
    "                        object1[\"object_id\"] = oid1\n",
    "\n",
    "                        rel_samples_positive.append({\n",
    "                            \"question_id\": qid,\n",
    "                            \"question\": question,\n",
    "                            \"image_id\": question[\"imageId\"],\n",
    "                            \"rel\": relation_type,\n",
    "                            \"object0\": object0,\n",
    "                            \"object1\": object1,\n",
    "                            \"object0_size\": compute_object_size(scene_graph, object0),\n",
    "                            \"object1_size\": compute_object_size(scene_graph, object1),\n",
    "                            \"y\": True\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/metadata/gqa_all_class.json') as f:\n",
    "    categories = json.load(f)\n",
    "class_to_category = {}\n",
    "for category, classes in categories.items():\n",
    "    for c in classes:\n",
    "        if c not in class_to_category:\n",
    "            class_to_category[c] = [category]\n",
    "        else:\n",
    "            class_to_category[c].append(category)\n",
    "\n",
    "class_samples_negative = []\n",
    "for sample in class_samples_positive:\n",
    "    candidate = random.choice(class_samples_positive)\n",
    "    while sample[\"class\"] in [*class_to_category.get(candidate[\"object\"][\"name\"], []), candidate[\"object\"][\"name\"]]:\n",
    "        candidate = random.choice(class_samples_positive)\n",
    "    class_samples_negative.append({\n",
    "        **candidate,\n",
    "        \"class\": sample[\"class\"],\n",
    "        \"y\": False\n",
    "    })\n",
    "class_samples = [*class_samples_positive, *class_samples_negative]\n",
    "random.shuffle(class_samples)\n",
    "\n",
    "attr_samples_negative = []\n",
    "for sample in attr_samples_positive:\n",
    "    candidate = random.choice(attr_samples_positive)\n",
    "    while sample[\"attr_value\"] in candidate[\"object\"][\"attributes\"]:\n",
    "        candidate = random.choice(attr_samples_positive)\n",
    "    \n",
    "    attr_samples_negative.append({\n",
    "        **candidate,\n",
    "        \"attr_value\": sample[\"attr_value\"],\n",
    "        \"y\": False\n",
    "    })\n",
    "attr_samples = [*attr_samples_positive, *attr_samples_negative]\n",
    "random.shuffle(attr_samples)\n",
    "\n",
    "rel_samples_negative = []\n",
    "for sample in rel_samples_positive:\n",
    "    candidate = random.choice(rel_samples_positive)\n",
    "    while any(r for r in candidate[\"object0\"][\"relations\"] if r[\"name\"] == sample[\"rel\"] and r[\"object\"] == candidate[\"object1\"][\"object_id\"]):\n",
    "        candidate = random.choice(rel_samples_positive)\n",
    "    \n",
    "    rel_samples_negative.append({\n",
    "        **candidate,\n",
    "        \"rel\": sample[\"rel\"],\n",
    "        \"y\": False\n",
    "    })\n",
    "rel_samples = [*rel_samples_positive, *rel_samples_negative]\n",
    "random.shuffle(rel_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class_samples_flat = [{\n",
    "    \"question_id\": s[\"question_id\"],\n",
    "    \"image_id\": s[\"image_id\"],\n",
    "    \"bbox_x\": s[\"object\"][\"x\"],\n",
    "    \"bbox_y\": s[\"object\"][\"y\"],\n",
    "    \"bbox_w\": s[\"object\"][\"w\"],\n",
    "    \"bbox_h\": s[\"object\"][\"h\"],\n",
    "    \"bbox_size\": s[\"object_size\"],\n",
    "    \"class\": s[\"class\"],\n",
    "    \"y\": s[\"y\"]\n",
    "} for s in class_samples]\n",
    "class_samples_df = pd.DataFrame.from_dict(class_samples_flat)\n",
    "\n",
    "attr_samples_flat = [{\n",
    "    \"question_id\": s[\"question_id\"],\n",
    "    \"image_id\": s[\"image_id\"],\n",
    "    \"bbox_x\": s[\"object\"][\"x\"],\n",
    "    \"bbox_y\": s[\"object\"][\"y\"],\n",
    "    \"bbox_w\": s[\"object\"][\"w\"],\n",
    "    \"bbox_h\": s[\"object\"][\"h\"],\n",
    "    \"bbox_size\": s[\"object_size\"],\n",
    "    \"object_name\": s[\"object\"][\"name\"],\n",
    "    \"attr_value\": s[\"attr_value\"],\n",
    "    \"y\": s[\"y\"]\n",
    "} for s in attr_samples]\n",
    "attr_samples_df = pd.DataFrame.from_dict(attr_samples_flat)\n",
    "\n",
    "rel_samples_flat = []\n",
    "for s in rel_samples:\n",
    "    object0, object1 = s['object0'], s['object1']\n",
    "    joined_bbox = {\n",
    "        \"y\": min(object0['y'], object1['y']),\n",
    "        \"x\": min(object0['x'], object1['x']),\n",
    "        \"h\": max(object0['y'] + object0['h'], object1['y'] + object1['h']) - min(object0['y'], object1['y']),\n",
    "        \"w\": max(object0['x'] + object0['w'], object1['x'] + object1['w']) - min(object0['x'], object1['x']),\n",
    "    }\n",
    "    rel_samples_flat.append({\n",
    "        \"question_id\": s[\"question_id\"],\n",
    "        \"image_id\": s[\"image_id\"],\n",
    "        \"bbox_x\": joined_bbox[\"x\"],\n",
    "        \"bbox_y\": joined_bbox[\"y\"],\n",
    "        \"bbox_w\": joined_bbox[\"w\"],\n",
    "        \"bbox_h\": joined_bbox[\"h\"],\n",
    "        \"bbox_size\": compute_object_size(scene_graphs[s[\"question\"][\"imageId\"]], joined_bbox),\n",
    "        \"object0_name\": s[\"object0\"][\"name\"],\n",
    "        \"object1_name\": s[\"object1\"][\"name\"],\n",
    "        \"rel\": s[\"rel\"],\n",
    "        \"y\": s[\"y\"]\n",
    "    })\n",
    "rel_samples_df = pd.DataFrame.from_dict(rel_samples_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97246"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_samples_df.to_pickle(\"../data/finetuning/val_attr_samples_10k.pkl\")\n",
    "rel_samples_df.to_pickle(\"../data/finetuning/val_rel_samples_10k.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    gpu = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"Warning: no GPU detected, falling back to CPU\")\n",
    "    gpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/jhadl/.conda/envs/jhadl_tf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Clean old model from cache\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from model.clip_model import CLIPModel\n",
    "model = CLIPModel(gpu, snapshot=\"model_snapshots/clip_finetune/checkpoint-66000\")\n",
    "\n",
    "# from model.xvlm_itr_coco_model import XVLMModel\n",
    "# model = XVLMModel(gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target vs. Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "class_dataset = PromptDataset(class_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a photo of a {e['class']}\", \n",
    "    \"a photo of an object\"\n",
    "], img_size=model.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.193%\n",
      "Precision: 0.883\n",
      "Recall: 0.881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': tensor(88.1928, device='cuda:0'),\n",
       " 'precision': tensor(0.8830, device='cuda:0'),\n",
       " 'recall': tensor(0.8805, device='cuda:0')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluator import Evaluator\n",
    "\n",
    "class_evaluator = Evaluator(model, class_dataset, batch_size=128)\n",
    "class_evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "def get_article(name):\n",
    "    return \"an\" if any(name.startswith(v) for v in [\"a\", \"e\", \"i\", \"o\", \"u\"]) else \"a\"\n",
    "\n",
    "attr_datasets = {\n",
    "    \"a bad photo of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a bad photo of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a bad photo of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a blurry photo of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a blurry photo of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a blurry photo of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a pixelated photo of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a pixelated photo of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a pixelated photo of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a low resolution photo of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a low resolution photo of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a low resolution photo of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a photo of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a photo of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a photo of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"{get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"{get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"nothing\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"{e['attr_value']} {e['object_name']}\", \n",
    "        f\"{e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"itap of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"itap of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"itap of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a bad picture of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a bad picture of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a bad picture of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a blurry picture of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a blurry picture of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a blurry picture of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a pixelated picture of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a pixelated picture of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a pixelated picture of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a low resolution picture of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a low resolution picture of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a low resolution picture of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a picture of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a picture of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a picture of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\")\n",
    "}\n",
    "\n",
    "\n",
    "attr_dataset = PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a bad photo of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "    f\"a bad photo of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "], img_size=model.img_size, mode=\"pad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a bad photo of an overcast sky', 'a bad photo of a sky']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(attr_dataset))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt format 'a bad photo of a'\n",
      "Accuracy: 67.929%\n",
      "Precision: 0.727\n",
      "Recall: 0.574\n",
      "Evaluating prompt format 'a blurry photo of a'\n",
      "Accuracy: 68.187%\n",
      "Precision: 0.712\n",
      "Recall: 0.611\n",
      "Evaluating prompt format 'a pixelated photo of a'\n",
      "Accuracy: 68.676%\n",
      "Precision: 0.686\n",
      "Recall: 0.690\n",
      "Evaluating prompt format 'a low resolution photo of a'\n",
      "Accuracy: 67.637%\n",
      "Precision: 0.708\n",
      "Recall: 0.600\n",
      "Evaluating prompt format 'a photo of a'\n",
      "Accuracy: 68.337%\n",
      "Precision: 0.719\n",
      "Recall: 0.603\n",
      "Evaluating prompt format 'a'\n",
      "Accuracy: 67.991%\n",
      "Precision: 0.737\n",
      "Recall: 0.560\n",
      "Evaluating prompt format 'nothing'\n",
      "Accuracy: 67.608%\n",
      "Precision: 0.732\n",
      "Recall: 0.555\n",
      "Evaluating prompt format 'itap of a'\n",
      "Accuracy: 67.725%\n",
      "Precision: 0.702\n",
      "Recall: 0.616\n",
      "Evaluating prompt format 'a bad picture of a'\n",
      "Accuracy: 67.830%\n",
      "Precision: 0.723\n",
      "Recall: 0.578\n",
      "Evaluating prompt format 'a blurry picture of a'\n",
      "Accuracy: 68.064%\n",
      "Precision: 0.710\n",
      "Recall: 0.611\n",
      "Evaluating prompt format 'a pixelated picture of a'\n",
      "Accuracy: 68.825%\n",
      "Precision: 0.683\n",
      "Recall: 0.702\n",
      "Evaluating prompt format 'a low resolution picture of a'\n",
      "Accuracy: 67.783%\n",
      "Precision: 0.708\n",
      "Recall: 0.606\n",
      "Evaluating prompt format 'a picture of a'\n",
      "Accuracy: 68.829%\n",
      "Precision: 0.712\n",
      "Recall: 0.631\n"
     ]
    }
   ],
   "source": [
    "import reload_recursive\n",
    "\n",
    "%reload evaluator\n",
    "from evaluator import Evaluator\n",
    "\n",
    "attr_results = {}\n",
    "for key, dataset in attr_datasets.items():\n",
    "    print(f\"Evaluating prompt format '{key}'\")\n",
    "    attr_evaluator = Evaluator(model, dataset, batch_size=128)\n",
    "    results = attr_evaluator.evaluate()\n",
    "    attr_results[key] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_results1 = [{\"schema\": k, **{m: mv.item() for m,mv in v.items()} }  for k,v in attr_results.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "schema & accuracy & precision & recall \\\\\n",
      "a bad photo of a & 0.6583 & 0.6728 & 0.6165 \\\\\n",
      "a blurry photo of a & 0.6590 & 0.6620 & 0.6495 \\\\\n",
      "a pixelated photo of a & 0.6512 & 0.6492 & 0.6577 \\\\\n",
      "a low resolution photo of a & 0.6555 & 0.6572 & 0.6499 \\\\\n",
      "a photo of a & 0.6522 & 0.6621 & 0.6215 \\\\\n",
      "a & 0.6533 & 0.6650 & 0.6178 \\\\\n",
      "nothing & 0.6465 & 0.6776 & 0.5591 \\\\\n",
      "itap of a & 0.6529 & 0.6741 & 0.5918 \\\\\n",
      "a bad picture of a & 0.6561 & 0.6673 & 0.6227 \\\\\n",
      "a blurry picture of a & 0.6549 & 0.6564 & 0.6501 \\\\\n",
      "a pixelated picture of a & 0.6551 & 0.6518 & 0.6660 \\\\\n",
      "a low resolution picture of a & 0.6571 & 0.6593 & 0.6502 \\\\\n",
      "a picture of a & 0.6560 & 0.6568 & 0.6534 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "attr_results_pd = pd.DataFrame(attr_results1)\n",
    "attr_results_pd[\"accuracy\"] = attr_results_pd[\"accuracy\"]/100\n",
    "\n",
    "attr_results_pd = attr_results_pd.style.format(precision=4).hide(axis=\"index\")\n",
    "print(attr_results_pd.to_latex())\n",
    "\n",
    "#attr_results_pd.to_pickle(\"prompt_engineering_results/attributes.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "def get_article(name):\n",
    "    return \"an\" if any(name.startswith(v) for v in [\"a\", \"e\", \"i\", \"o\", \"u\"]) else \"a\"\n",
    "\n",
    "rel_datasets = {\n",
    "    \"a bad photo of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a bad photo of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a bad photo of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a blurry photo of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a blurry photo of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a blurry photo of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a pixelated photo of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a pixelated photo of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a pixelated photo of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a low resolution photo of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a low resolution photo of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a low resolution photo of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a photo of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a photo of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a photo of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"{get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"{get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"nothing\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"{e['object0_name']} {e['rel']} {e['object1_name']}\", \n",
    "        f\"{e['object0_name']} and {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"itap of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"itap of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"itap of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a bad picture of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a bad picture of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a bad picture of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a blurry picture of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a blurry picture of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a blurry picture of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a pixelated picture of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a pixelated picture of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a pixelated picture of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a low resolution picture of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a low resolution picture of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a low resolution picture of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a picture of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a picture of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a picture of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\")\n",
    "}\n",
    "\n",
    "rel_dataset = PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a bad photo of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "    f\"a bad photo of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "], img_size=model.img_size, mode=\"pad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt format 'a bad photo of a'\n",
      "Accuracy: 54.688%\n",
      "Precision: 0.538\n",
      "Recall: 0.657\n",
      "Evaluating prompt format 'a blurry photo of a'\n",
      "Accuracy: 53.798%\n",
      "Precision: 0.529\n",
      "Recall: 0.694\n",
      "Evaluating prompt format 'a pixelated photo of a'\n",
      "Accuracy: 55.334%\n",
      "Precision: 0.549\n",
      "Recall: 0.592\n",
      "Evaluating prompt format 'a low resolution photo of a'\n",
      "Accuracy: 54.822%\n",
      "Precision: 0.548\n",
      "Recall: 0.554\n",
      "Evaluating prompt format 'a photo of a'\n",
      "Accuracy: 55.444%\n",
      "Precision: 0.543\n",
      "Recall: 0.688\n",
      "Evaluating prompt format 'a'\n",
      "Accuracy: 54.933%\n",
      "Precision: 0.537\n",
      "Recall: 0.721\n",
      "Evaluating prompt format 'nothing'\n",
      "Accuracy: 56.321%\n",
      "Precision: 0.553\n",
      "Recall: 0.659\n",
      "Evaluating prompt format 'itap of a'\n",
      "Accuracy: 54.148%\n",
      "Precision: 0.540\n",
      "Recall: 0.556\n",
      "Evaluating prompt format 'a bad picture of a'\n",
      "Accuracy: 54.310%\n",
      "Precision: 0.536\n",
      "Recall: 0.643\n",
      "Evaluating prompt format 'a blurry picture of a'\n",
      "Accuracy: 53.951%\n",
      "Precision: 0.532\n",
      "Recall: 0.666\n",
      "Evaluating prompt format 'a pixelated picture of a'\n",
      "Accuracy: 55.240%\n",
      "Precision: 0.549\n",
      "Recall: 0.583\n",
      "Evaluating prompt format 'a low resolution picture of a'\n",
      "Accuracy: 54.994%\n",
      "Precision: 0.550\n",
      "Recall: 0.554\n",
      "Evaluating prompt format 'a picture of a'\n",
      "Accuracy: 55.826%\n",
      "Precision: 0.547\n",
      "Recall: 0.680\n"
     ]
    }
   ],
   "source": [
    "%reload evaluator\n",
    "from evaluator import Evaluator\n",
    "\n",
    "rel_results = {}\n",
    "for key, dataset in rel_datasets.items():\n",
    "    print(f\"Evaluating prompt format '{key}'\")\n",
    "    rel_evaluator = Evaluator(model, dataset, batch_size=128)\n",
    "    results = rel_evaluator.evaluate()\n",
    "    rel_results[key] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "schema & accuracy & precision & recall \\\\\n",
      "a bad photo of a & 0.5469 & 0.5384 & 0.6569 \\\\\n",
      "a blurry photo of a & 0.5380 & 0.5290 & 0.6939 \\\\\n",
      "a pixelated photo of a & 0.5533 & 0.5495 & 0.5922 \\\\\n",
      "a low resolution photo of a & 0.5482 & 0.5476 & 0.5542 \\\\\n",
      "a photo of a & 0.5544 & 0.5430 & 0.6880 \\\\\n",
      "a & 0.5493 & 0.5367 & 0.7213 \\\\\n",
      "nothing & 0.5632 & 0.5530 & 0.6590 \\\\\n",
      "itap of a & 0.5415 & 0.5403 & 0.5562 \\\\\n",
      "a bad picture of a & 0.5431 & 0.5359 & 0.6429 \\\\\n",
      "a blurry picture of a & 0.5395 & 0.5316 & 0.6656 \\\\\n",
      "a pixelated picture of a & 0.5524 & 0.5493 & 0.5834 \\\\\n",
      "a low resolution picture of a & 0.5499 & 0.5495 & 0.5541 \\\\\n",
      "a picture of a & 0.5583 & 0.5469 & 0.6799 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rel_results1 = [{\"schema\": k, **{m: mv.item() for m,mv in v.items()} }  for k,v in rel_results.items()]\n",
    "rel_results_pd = pd.DataFrame(rel_results1)\n",
    "rel_results_pd[\"accuracy\"] = rel_results_pd[\"accuracy\"]/100\n",
    "\n",
    "rel_results_pd = rel_results_pd.style.format(precision=4).hide(axis=\"index\")\n",
    "print(rel_results_pd.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target vs. Contrastive Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/metadata/gqa_all_class.json\") as f:\n",
    "    classes = json.load(f)\n",
    "    \n",
    "with open(\"../data/metadata/gqa_all_attribute.json\") as f:\n",
    "    attributes = json.load(f)\n",
    "    \n",
    "with open(\"../data/metadata/gqa_relation.json\") as f:\n",
    "    relations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "classes_sample = random.sample([item.replace('_', ' ') for items in classes.values() for item in items], 100)\n",
    "attributes_sample = random.sample([item.replace('_', ' ') for items in attributes.values() for item in items], 100)\n",
    "rels_sample = random.sample([item.replace('_', ' ') for item in relations], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_prompts = [f\"a bad photo of a {clazz}\" for clazz in classes_sample]\n",
    "attr_prompts = [f\"a bad photo of a {attr} object\" for attr in attributes_sample]\n",
    "rel_prompts = [f\"a bad photo of an object {rel} an object\" for rel in rels_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "class_dataset = PromptDataset(class_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a bad photo of a {e['class']}\", \n",
    "    \"a bad photo of an object\"\n",
    "], img_size=model.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classes_prompts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mevaluator\u001b[39;00m \u001b[39mimport\u001b[39;00m ContrastiveEvaluator\n\u001b[0;32m----> 3\u001b[0m class_evaluator \u001b[39m=\u001b[39m ContrastiveEvaluator(model, class_dataset, classes_prompts, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m)\n\u001b[1;32m      4\u001b[0m class_evaluator\u001b[39m.\u001b[39mevaluate()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classes_prompts' is not defined"
     ]
    }
   ],
   "source": [
    "from evaluator import ContrastiveEvaluator\n",
    "\n",
    "class_evaluator = ContrastiveEvaluator(model, class_dataset, classes_prompts, batch_size=64)\n",
    "class_evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "attr_dataset = PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a bad photo of a {e['attr_value']} object\", \n",
    "    \"a bad photo of an regular object\"\n",
    "], img_size=model.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.668%\n",
      "Precision: 0.567\n",
      "Recall: 0.564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 56.66811468288445,\n",
       " 'precision': 0.5670110593713621,\n",
       " 'recall': 0.5642195192586157}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluator import ContrastiveEvaluator\n",
    "\n",
    "attr_evaluator = ContrastiveEvaluator(model, attr_dataset, attr_prompts, batch_size=64)\n",
    "attr_evaluator.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "35ab78d29ed88d957c44548976856795b78553768444957779fbd79cbca61a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
