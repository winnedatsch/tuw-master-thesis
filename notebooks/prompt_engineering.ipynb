{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# sys path hack to allow importing the encoding functions and other modules\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "sys.path.insert(0, os.path.abspath('../externals'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/questions/val_sampled_questions_50000.json\") as f:\n",
    "   questions = list(json.load(f).items())\n",
    "\n",
    "with open(\"../data/sceneGraphs/val_sceneGraphs.json\") as f:\n",
    "    scene_graphs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def compute_object_size(scene_graph, object):\n",
    "    image_size = scene_graph[\"width\"] * scene_graph[\"height\"]\n",
    "    object_size = object[\"w\"] * object[\"h\"]\n",
    "    return object_size / image_size\n",
    "\n",
    "class_samples_positive = []\n",
    "attr_samples_positive = []\n",
    "rel_samples_positive = []\n",
    "\n",
    "def object_within_image_bounds(scene_graph, object):\n",
    "    return object[\"x\"] >= 0 and object[\"y\"] >= 0 and object[\"h\"] > 0 and object[\"w\"] > 0 and \\\n",
    "           object[\"x\"] + object[\"w\"] <= scene_graph[\"width\"] and object[\"y\"] + object[\"h\"] <= scene_graph[\"height\"]\n",
    "\n",
    "for qid, question in questions:\n",
    "    scene_graph = scene_graphs[question[\"imageId\"]]\n",
    "    for op in question[\"semantic\"]:\n",
    "        operation = op[\"operation\"]\n",
    "        argument = op[\"argument\"].strip()\n",
    "        objects = scene_graph[\"objects\"]\n",
    "\n",
    "        if operation == \"select\" and argument != \"scene\" and not argument.endswith(\"(-)\"):\n",
    "            matching_objects = [(oid, objects[oid]) for oid in argument.split(\"(\")[1][:-1].split(\",\") if object_within_image_bounds(scene_graph, objects[oid])] \n",
    "            if len(matching_objects) > 0:\n",
    "                oid, object = random.choice(matching_objects)\n",
    "                object[\"object_id\"] = oid\n",
    "                class_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"class\": argument.split(\"(\")[0].strip(),\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(scene_graph, object),\n",
    "                        \"y\": True\n",
    "                    })  \n",
    "\n",
    "        elif operation.startswith(\"filter\"):\n",
    "            attr = ' '.join(operation.split(' ')[1:]) if operation != \"filter\" else \"any\"\n",
    "            attr_value = argument[4:-1] if argument.startswith('not(') else argument\n",
    "            objects_with_attr = [(oid, o) for oid, o in objects.items() if attr_value in o[\"attributes\"] and object_within_image_bounds(scene_graph,o)]\n",
    "            if len(objects_with_attr) > 0 and attr not in [\"hposition\", \"vposition\"]:\n",
    "                oid, object = random.choice(objects_with_attr)\n",
    "                object[\"object_id\"] = oid\n",
    "                attr_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"attr_value\": attr_value,\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(scene_graph, object),\n",
    "                        \"y\": True\n",
    "                    })\n",
    "\n",
    "        elif operation.startswith(\"verify\"):\n",
    "            attr = ' '.join(operation.split(' ')[1:]) if operation != \"verify\" else \"any\"\n",
    "            attr_value = argument\n",
    "            objects_with_attr = [(oid, o) for oid, o in objects.items() if attr_value in o[\"attributes\"] and object_within_image_bounds(scene_graph, o)]\n",
    "            if len(objects_with_attr) > 0 and attr not in [\"hposition\", \"vposition\"]:\n",
    "                oid, object = random.choice(objects_with_attr)\n",
    "                object[\"object_id\"] = oid\n",
    "                attr_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"attr_value\": attr_value,\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(scene_graph, object),\n",
    "                        \"y\": True\n",
    "                    })\n",
    "                \n",
    "        elif operation.startswith(\"choose \") and argument != \"\":\n",
    "            attr = \" \".join(operation.split(\" \")[1:])\n",
    "            attr_value = random.choice([argument.split(\"|\")[0], argument.split(\"|\")[1]])\n",
    "            objects_with_attr = [(oid, o) for oid, o in objects.items() if attr_value in o[\"attributes\"] and object_within_image_bounds(scene_graph, o)]\n",
    "            if len(objects_with_attr) > 0 and attr not in [\"hposition\", \"vposition\"]:\n",
    "                oid, object = random.choice(objects_with_attr)\n",
    "                object[\"object_id\"] = oid\n",
    "                attr_samples_positive.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"question\": question,\n",
    "                        \"image_id\": question[\"imageId\"],\n",
    "                        \"attr_value\": attr_value,\n",
    "                        \"object\": object,\n",
    "                        \"object_size\": compute_object_size(scene_graph, object),\n",
    "                        \"y\": True\n",
    "                    })\n",
    "                \n",
    "        elif operation == \"relate\":\n",
    "            relation_type = argument.split(',')[1]\n",
    "            position = 'subject' if argument.split(',')[2].startswith('s') else 'object'\n",
    "            target_object = argument.split('(')[1][:-1]\n",
    "\n",
    "            if target_object != \"-\":\n",
    "                if position == 'object':\n",
    "                    matching_objects = [(oid, o) for oid, o in objects.items() if any(r[\"object\"] == target_object and r[\"name\"] == relation_type for r in o[\"relations\"]) and object_within_image_bounds(scene_graph, o)]\n",
    "                    if len(matching_objects) > 0:\n",
    "                        oid0, object0 = random.choice(matching_objects)\n",
    "                        object0[\"object_id\"] = oid0\n",
    "                        object1 = objects[target_object]\n",
    "                        object1[\"object_id\"] = target_object\n",
    "\n",
    "                        rel_samples_positive.append({\n",
    "                                \"question_id\": qid,\n",
    "                                \"question\": question,\n",
    "                                \"image_id\": question[\"imageId\"],\n",
    "                                \"rel\": relation_type,\n",
    "                                \"object0\": object0,\n",
    "                                \"object1\": object1,\n",
    "                                \"object0_size\": compute_object_size(scene_graph, object0),\n",
    "                                \"object1_size\": compute_object_size(scene_graph, object1),\n",
    "                                \"y\": True\n",
    "                            })\n",
    "                        \n",
    "                else:\n",
    "                    matching_oids = [r[\"object\"] for r in objects[target_object][\"relations\"] if r[\"name\"] == relation_type]\n",
    "                    matching_objects = [(oid, objects[oid]) for oid in matching_oids if object_within_image_bounds(scene_graph, objects[oid])]\n",
    "                    if len(matching_objects) > 0:\n",
    "                        object0 = objects[target_object]\n",
    "                        object0[\"object_id\"] = target_object\n",
    "                        oid1, object1 = random.choice(matching_objects)\n",
    "                        object1[\"object_id\"] = oid1\n",
    "\n",
    "                        rel_samples_positive.append({\n",
    "                            \"question_id\": qid,\n",
    "                            \"question\": question,\n",
    "                            \"image_id\": question[\"imageId\"],\n",
    "                            \"rel\": relation_type,\n",
    "                            \"object0\": object0,\n",
    "                            \"object1\": object1,\n",
    "                            \"object0_size\": compute_object_size(scene_graph, object0),\n",
    "                            \"object1_size\": compute_object_size(scene_graph, object1),\n",
    "                            \"y\": True\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/metadata/gqa_all_class.json') as f:\n",
    "    categories = json.load(f)\n",
    "class_to_category = {}\n",
    "for category, classes in categories.items():\n",
    "    for c in classes:\n",
    "        if c not in class_to_category:\n",
    "            class_to_category[c] = [category]\n",
    "        else:\n",
    "            class_to_category[c].append(category)\n",
    "\n",
    "class_samples_negative = []\n",
    "for sample in class_samples_positive:\n",
    "    candidate = random.choice(class_samples_positive)\n",
    "    while sample[\"class\"] in [*class_to_category.get(candidate[\"object\"][\"name\"], []), candidate[\"object\"][\"name\"]]:\n",
    "        candidate = random.choice(class_samples_positive)\n",
    "    class_samples_negative.append({\n",
    "        **candidate,\n",
    "        \"class\": sample[\"class\"],\n",
    "        \"y\": False\n",
    "    })\n",
    "class_samples = [*class_samples_positive, *class_samples_negative]\n",
    "random.shuffle(class_samples)\n",
    "\n",
    "attr_samples_negative = []\n",
    "for sample in attr_samples_positive:\n",
    "    candidate = random.choice(attr_samples_positive)\n",
    "    while sample[\"attr_value\"] in candidate[\"object\"][\"attributes\"]:\n",
    "        candidate = random.choice(attr_samples_positive)\n",
    "    \n",
    "    attr_samples_negative.append({\n",
    "        **candidate,\n",
    "        \"attr_value\": sample[\"attr_value\"],\n",
    "        \"y\": False\n",
    "    })\n",
    "attr_samples = [*attr_samples_positive, *attr_samples_negative]\n",
    "random.shuffle(attr_samples)\n",
    "\n",
    "rel_samples_negative = []\n",
    "for sample in rel_samples_positive:\n",
    "    candidate = random.choice(rel_samples_positive)\n",
    "    while any(r for r in candidate[\"object0\"][\"relations\"] if r[\"name\"] == sample[\"rel\"] and r[\"object\"] == candidate[\"object1\"][\"object_id\"]):\n",
    "        candidate = random.choice(rel_samples_positive)\n",
    "    \n",
    "    rel_samples_negative.append({\n",
    "        **candidate,\n",
    "        \"rel\": sample[\"rel\"],\n",
    "        \"y\": False\n",
    "    })\n",
    "rel_samples = [*rel_samples_positive, *rel_samples_negative]\n",
    "random.shuffle(rel_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class_samples_flat = [{\n",
    "    \"question_id\": s[\"question_id\"],\n",
    "    \"image_id\": s[\"image_id\"],\n",
    "    \"bbox_x\": s[\"object\"][\"x\"],\n",
    "    \"bbox_y\": s[\"object\"][\"y\"],\n",
    "    \"bbox_w\": s[\"object\"][\"w\"],\n",
    "    \"bbox_h\": s[\"object\"][\"h\"],\n",
    "    \"bbox_size\": s[\"object_size\"],\n",
    "    \"class\": s[\"class\"],\n",
    "    \"y\": s[\"y\"]\n",
    "} for s in class_samples]\n",
    "class_samples_df = pd.DataFrame.from_dict(class_samples_flat)\n",
    "\n",
    "attr_samples_flat = [{\n",
    "    \"question_id\": s[\"question_id\"],\n",
    "    \"image_id\": s[\"image_id\"],\n",
    "    \"bbox_x\": s[\"object\"][\"x\"],\n",
    "    \"bbox_y\": s[\"object\"][\"y\"],\n",
    "    \"bbox_w\": s[\"object\"][\"w\"],\n",
    "    \"bbox_h\": s[\"object\"][\"h\"],\n",
    "    \"bbox_size\": s[\"object_size\"],\n",
    "    \"object_name\": s[\"object\"][\"name\"],\n",
    "    \"attr_value\": s[\"attr_value\"],\n",
    "    \"y\": s[\"y\"]\n",
    "} for s in attr_samples]\n",
    "attr_samples_df = pd.DataFrame.from_dict(attr_samples_flat)\n",
    "\n",
    "rel_samples_flat = []\n",
    "for s in rel_samples:\n",
    "    object0, object1 = s['object0'], s['object1']\n",
    "    joined_bbox = {\n",
    "        \"y\": min(object0['y'], object1['y']),\n",
    "        \"x\": min(object0['x'], object1['x']),\n",
    "        \"h\": max(object0['y'] + object0['h'], object1['y'] + object1['h']) - min(object0['y'], object1['y']),\n",
    "        \"w\": max(object0['x'] + object0['w'], object1['x'] + object1['w']) - min(object0['x'], object1['x']),\n",
    "    }\n",
    "    rel_samples_flat.append({\n",
    "        \"question_id\": s[\"question_id\"],\n",
    "        \"image_id\": s[\"image_id\"],\n",
    "        \"bbox_x\": joined_bbox[\"x\"],\n",
    "        \"bbox_y\": joined_bbox[\"y\"],\n",
    "        \"bbox_w\": joined_bbox[\"w\"],\n",
    "        \"bbox_h\": joined_bbox[\"h\"],\n",
    "        \"bbox_size\": compute_object_size(scene_graphs[s[\"question\"][\"imageId\"]], joined_bbox),\n",
    "        \"object0_name\": s[\"object0\"][\"name\"],\n",
    "        \"object1_name\": s[\"object1\"][\"name\"],\n",
    "        \"rel\": s[\"rel\"],\n",
    "        \"y\": s[\"y\"]\n",
    "    })\n",
    "rel_samples_df = pd.DataFrame.from_dict(rel_samples_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97246"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m attr_samples_df\u001b[39m.\u001b[39mto_pickle(\u001b[39m\"\u001b[39m\u001b[39m../data/finetuning/val_attr_samples_10k.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m rel_samples_df\u001b[39m.\u001b[39;49mto_pickle(\u001b[39m\"\u001b[39;49m\u001b[39m../data/finetuning/val_rel_samples_10k.pkl\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/pandas/core/generic.py:2955\u001b[0m, in \u001b[0;36mNDFrame.to_pickle\u001b[0;34m(self, path, compression, protocol, storage_options)\u001b[0m\n\u001b[1;32m   2903\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2904\u001b[0m \u001b[39mPickle (serialize) object to file.\u001b[39;00m\n\u001b[1;32m   2905\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2951\u001b[0m \u001b[39m4    4    9\u001b[39;00m\n\u001b[1;32m   2952\u001b[0m \u001b[39m\"\"\"\u001b[39;00m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m   2953\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpickle\u001b[39;00m \u001b[39mimport\u001b[39;00m to_pickle\n\u001b[0;32m-> 2955\u001b[0m to_pickle(\n\u001b[1;32m   2956\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2957\u001b[0m     path,\n\u001b[1;32m   2958\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2959\u001b[0m     protocol\u001b[39m=\u001b[39;49mprotocol,\n\u001b[1;32m   2960\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2961\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/pandas/io/pickle.py:103\u001b[0m, in \u001b[0;36mto_pickle\u001b[0;34m(obj, filepath_or_buffer, compression, protocol, storage_options)\u001b[0m\n\u001b[1;32m     93\u001b[0m     protocol \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mHIGHEST_PROTOCOL\n\u001b[1;32m     95\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m     96\u001b[0m     filepath_or_buffer,\n\u001b[1;32m     97\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    102\u001b[0m     \u001b[39m# letting pickle write directly to the buffer is more memory-efficient\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     pickle\u001b[39m.\u001b[39;49mdump(obj, handles\u001b[39m.\u001b[39;49mhandle, protocol\u001b[39m=\u001b[39;49mprotocol)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "attr_samples_df.to_pickle(\"../data/finetuning/val_attr_samples_10k.pkl\")\n",
    "rel_samples_df.to_pickle(\"../data/finetuning/val_rel_samples_10k.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    gpu = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"Warning: no GPU detected, falling back to CPU\")\n",
    "    gpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean old model from cache\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from model.clip_model import CLIPModel\n",
    "model = CLIPModel(gpu, model=\"openai/clip-vit-base-patch32\", snapshot=\"model_snapshots/clip_finetune/checkpoint-66000\")\n",
    "\n",
    "# from model.xvlm_itr_coco_model import XVLMModel\n",
    "# model = XVLMModel(gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target vs. Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "class_dataset = PromptDataset(class_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a photo of a {e['class']}\", \n",
    "    \"a photo of an object\"\n",
    "], img_size=model.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.193%\n",
      "Precision: 0.883\n",
      "Recall: 0.881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': tensor(88.1928, device='cuda:0'),\n",
       " 'precision': tensor(0.8830, device='cuda:0'),\n",
       " 'recall': tensor(0.8805, device='cuda:0')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluator import Evaluator\n",
    "\n",
    "class_evaluator = Evaluator(model, class_dataset, batch_size=128)\n",
    "class_evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "def get_article(name):\n",
    "    return \"an\" if any(name.startswith(v) for v in [\"a\", \"e\", \"i\", \"o\", \"u\"]) else \"a\"\n",
    "\n",
    "attr_datasets = {\n",
    "    \"a bad photo of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a bad photo of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a bad photo of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a blurry photo of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a blurry photo of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a blurry photo of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a pixelated photo of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a pixelated photo of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a pixelated photo of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a low resolution photo of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a low resolution photo of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a low resolution photo of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a photo of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a photo of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a photo of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"{get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"{get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"nothing\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"{e['attr_value']} {e['object_name']}\", \n",
    "        f\"{e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"itap of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"itap of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"itap of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a bad picture of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a bad picture of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a bad picture of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a blurry picture of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a blurry picture of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a blurry picture of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a pixelated picture of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a pixelated picture of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a pixelated picture of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a low resolution picture of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a low resolution picture of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a low resolution picture of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a picture of a\": PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a picture of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "        f\"a picture of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\")\n",
    "}\n",
    "\n",
    "\n",
    "attr_dataset = PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a bad photo of {get_article(e['attr_value'])} {e['attr_value']} {e['object_name']}\", \n",
    "    f\"a bad photo of {get_article(e['object_name'])} {e['object_name']}\"\n",
    "], img_size=model.img_size, mode=\"pad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a bad photo of an overcast sky', 'a bad photo of a sky']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(attr_dataset))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt format 'a bad photo of a'\n",
      "Accuracy: 67.863%\n",
      "Precision: 0.723\n",
      "Recall: 0.579\n",
      "Evaluating prompt format 'a blurry photo of a'\n",
      "Accuracy: 67.951%\n",
      "Precision: 0.707\n",
      "Recall: 0.614\n",
      "Evaluating prompt format 'a pixelated photo of a'\n",
      "Accuracy: 68.742%\n",
      "Precision: 0.685\n",
      "Recall: 0.693\n",
      "Evaluating prompt format 'a low resolution photo of a'\n",
      "Accuracy: 67.736%\n",
      "Precision: 0.708\n",
      "Recall: 0.604\n",
      "Evaluating prompt format 'a photo of a'\n",
      "Accuracy: 68.046%\n",
      "Precision: 0.712\n",
      "Recall: 0.606\n",
      "Evaluating prompt format 'a'\n",
      "Accuracy: 68.162%\n",
      "Precision: 0.739\n",
      "Recall: 0.562\n",
      "Evaluating prompt format 'nothing'\n",
      "Accuracy: 67.696%\n",
      "Precision: 0.733\n",
      "Recall: 0.557\n",
      "Evaluating prompt format 'itap of a'\n",
      "Accuracy: 67.871%\n",
      "Precision: 0.703\n",
      "Recall: 0.619\n",
      "Evaluating prompt format 'a bad picture of a'\n",
      "Accuracy: 67.732%\n",
      "Precision: 0.720\n",
      "Recall: 0.581\n",
      "Evaluating prompt format 'a blurry picture of a'\n",
      "Accuracy: 67.896%\n",
      "Precision: 0.705\n",
      "Recall: 0.615\n",
      "Evaluating prompt format 'a pixelated picture of a'\n",
      "Accuracy: 68.953%\n",
      "Precision: 0.684\n",
      "Recall: 0.705\n",
      "Evaluating prompt format 'a low resolution picture of a'\n",
      "Accuracy: 67.812%\n",
      "Precision: 0.706\n",
      "Recall: 0.610\n",
      "Evaluating prompt format 'a picture of a'\n",
      "Accuracy: 68.614%\n",
      "Precision: 0.707\n",
      "Recall: 0.635\n"
     ]
    }
   ],
   "source": [
    "import reload_recursive\n",
    "\n",
    "%reload evaluator\n",
    "from evaluator import Evaluator\n",
    "\n",
    "attr_results = {}\n",
    "for key, dataset in attr_datasets.items():\n",
    "    print(f\"Evaluating prompt format '{key}'\")\n",
    "    attr_evaluator = Evaluator(model, dataset, batch_size=256)\n",
    "    results = attr_evaluator.evaluate()\n",
    "    attr_results[key] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_results1 = [{\"schema\": k, **{m: mv.item() for m,mv in v.items()} }  for k,v in attr_results.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "schema & accuracy & precision & recall \\\\\n",
      "a bad photo of a & 67.8634 & 0.7232 & 0.5787 \\\\\n",
      "a blurry photo of a & 67.9509 & 0.7066 & 0.6139 \\\\\n",
      "a pixelated photo of a & 68.7418 & 0.6855 & 0.6927 \\\\\n",
      "a low resolution photo of a & 67.7358 & 0.7079 & 0.6040 \\\\\n",
      "a photo of a & 68.0456 & 0.7120 & 0.6060 \\\\\n",
      "a & 68.1623 & 0.7390 & 0.5615 \\\\\n",
      "nothing & 67.6957 & 0.7326 & 0.5573 \\\\\n",
      "itap of a & 67.8707 & 0.7031 & 0.6187 \\\\\n",
      "a bad picture of a & 67.7322 & 0.7197 & 0.5809 \\\\\n",
      "a blurry picture of a & 67.8962 & 0.7052 & 0.6150 \\\\\n",
      "a pixelated picture of a & 68.9532 & 0.6839 & 0.7048 \\\\\n",
      "a low resolution picture of a & 67.8124 & 0.7064 & 0.6096 \\\\\n",
      "a picture of a & 68.6142 & 0.7074 & 0.6349 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "attr_results_pd = pd.DataFrame(attr_results1)\n",
    "# attr_results_pd = pd.read_pickle(\"prompt_engineering_results/attributes_base_patch32.pkl\")\n",
    "# attr_results_pd.to_pickle(\"prompt_engineering_results/attributes_large_patch14.pkl\")\n",
    "attr_results_pd[\"accuracy\"] = attr_results_pd[\"accuracy\"]\n",
    "\n",
    "attr_results_pd = attr_results_pd.style.format(precision=4).hide(axis=\"index\")\n",
    "print(attr_results_pd.to_latex())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "def get_article(name):\n",
    "    return \"an\" if any(name.startswith(v) for v in [\"a\", \"e\", \"i\", \"o\", \"u\"]) else \"a\"\n",
    "\n",
    "rel_datasets = {\n",
    "    \"a bad photo of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a bad photo of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a bad photo of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a blurry photo of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a blurry photo of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a blurry photo of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a pixelated photo of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a pixelated photo of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a pixelated photo of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a low resolution photo of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a low resolution photo of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a low resolution photo of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a photo of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a photo of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a photo of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"{get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"{get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"nothing\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"{e['object0_name']} {e['rel']} {e['object1_name']}\", \n",
    "        f\"{e['object0_name']} and {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"itap of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"itap of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"itap of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a bad picture of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a bad picture of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a bad picture of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a blurry picture of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a blurry picture of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a blurry picture of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a pixelated picture of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a pixelated picture of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a pixelated picture of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a low resolution picture of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a low resolution picture of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a low resolution picture of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\"),\n",
    "    \"a picture of a\": PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "        f\"a picture of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "        f\"a picture of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "    ], img_size=model.img_size, mode=\"pad\")\n",
    "}\n",
    "\n",
    "rel_dataset = PromptDataset(rel_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a bad photo of {get_article(e['object0_name'])} {e['object0_name']} {e['rel']} {get_article(e['object1_name'])} {e['object1_name']}\", \n",
    "    f\"a bad photo of {get_article(e['object0_name'])} {e['object0_name']} and {get_article(e['object1_name'])} {e['object1_name']}\"\n",
    "], img_size=model.img_size, mode=\"pad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt format 'a bad photo of a'\n",
      "Accuracy: 58.311%\n",
      "Precision: 0.568\n",
      "Recall: 0.695\n",
      "Evaluating prompt format 'a blurry photo of a'\n",
      "Accuracy: 57.640%\n",
      "Precision: 0.559\n",
      "Recall: 0.729\n",
      "Evaluating prompt format 'a pixelated photo of a'\n",
      "Accuracy: 59.287%\n",
      "Precision: 0.588\n",
      "Recall: 0.619\n",
      "Evaluating prompt format 'a low resolution photo of a'\n",
      "Accuracy: 57.749%\n",
      "Precision: 0.577\n",
      "Recall: 0.581\n",
      "Evaluating prompt format 'a photo of a'\n",
      "Accuracy: 58.462%\n",
      "Precision: 0.570\n",
      "Recall: 0.690\n",
      "Evaluating prompt format 'a'\n",
      "Accuracy: 59.840%\n",
      "Precision: 0.578\n",
      "Recall: 0.733\n",
      "Evaluating prompt format 'nothing'\n",
      "Accuracy: 59.250%\n",
      "Precision: 0.578\n",
      "Recall: 0.688\n",
      "Evaluating prompt format 'itap of a'\n",
      "Accuracy: 57.039%\n",
      "Precision: 0.561\n",
      "Recall: 0.648\n",
      "Evaluating prompt format 'a bad picture of a'\n",
      "Accuracy: 58.393%\n",
      "Precision: 0.570\n",
      "Recall: 0.683\n",
      "Evaluating prompt format 'a blurry picture of a'\n",
      "Accuracy: 58.085%\n",
      "Precision: 0.566\n",
      "Recall: 0.695\n",
      "Evaluating prompt format 'a pixelated picture of a'\n",
      "Accuracy: 59.596%\n",
      "Precision: 0.593\n",
      "Recall: 0.614\n",
      "Evaluating prompt format 'a low resolution picture of a'\n",
      "Accuracy: 57.998%\n",
      "Precision: 0.585\n",
      "Recall: 0.552\n",
      "Evaluating prompt format 'a picture of a'\n",
      "Accuracy: 59.439%\n",
      "Precision: 0.582\n",
      "Recall: 0.667\n"
     ]
    }
   ],
   "source": [
    "import reload_recursive\n",
    "\n",
    "%reload evaluator\n",
    "from evaluator import Evaluator\n",
    "\n",
    "rel_results = {}\n",
    "for key, dataset in rel_datasets.items():\n",
    "    print(f\"Evaluating prompt format '{key}'\")\n",
    "    rel_evaluator = Evaluator(model, dataset, batch_size=128)\n",
    "    results = rel_evaluator.evaluate()\n",
    "    rel_results[key] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "schema & accuracy & precision & recall \\\\\n",
      "a bad photo of a & 0.5831 & 0.5679 & 0.6950 \\\\\n",
      "a blurry photo of a & 0.5764 & 0.5586 & 0.7286 \\\\\n",
      "a pixelated photo of a & 0.5929 & 0.5882 & 0.6193 \\\\\n",
      "a low resolution photo of a & 0.5775 & 0.5770 & 0.5805 \\\\\n",
      "a photo of a & 0.5846 & 0.5699 & 0.6901 \\\\\n",
      "a & 0.5984 & 0.5775 & 0.7331 \\\\\n",
      "nothing & 0.5925 & 0.5777 & 0.6880 \\\\\n",
      "itap of a & 0.5704 & 0.5610 & 0.6475 \\\\\n",
      "a bad picture of a & 0.5839 & 0.5700 & 0.6834 \\\\\n",
      "a blurry picture of a & 0.5808 & 0.5658 & 0.6954 \\\\\n",
      "a pixelated picture of a & 0.5960 & 0.5926 & 0.6139 \\\\\n",
      "a low resolution picture of a & 0.5800 & 0.5847 & 0.5524 \\\\\n",
      "a picture of a & 0.5944 & 0.5824 & 0.6672 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rel_results1 = [{\"schema\": k, **{m: mv.item() for m,mv in v.items()} }  for k,v in rel_results.items()]\n",
    "rel_results_pd = pd.DataFrame(rel_results1)\n",
    "rel_results_pd[\"accuracy\"] = rel_results_pd[\"accuracy\"]/100\n",
    "\n",
    "rel_results_pd = rel_results_pd.style.format(precision=4).hide(axis=\"index\")\n",
    "print(rel_results_pd.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target vs. Contrastive Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/metadata/gqa_all_class.json\") as f:\n",
    "    classes = json.load(f)\n",
    "    \n",
    "with open(\"../data/metadata/gqa_all_attribute.json\") as f:\n",
    "    attributes = json.load(f)\n",
    "    \n",
    "with open(\"../data/metadata/gqa_relation.json\") as f:\n",
    "    relations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "classes_sample = random.sample([item.replace('_', ' ') for items in classes.values() for item in items], 100)\n",
    "attributes_sample = random.sample([item.replace('_', ' ') for items in attributes.values() for item in items], 100)\n",
    "rels_sample = random.sample([item.replace('_', ' ') for item in relations], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_prompts = [f\"a bad photo of a {clazz}\" for clazz in classes_sample]\n",
    "attr_prompts = [f\"a bad photo of a {attr} object\" for attr in attributes_sample]\n",
    "rel_prompts = [f\"a bad photo of an object {rel} an object\" for rel in rels_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "class_dataset = PromptDataset(class_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a bad photo of a {e['class']}\", \n",
    "    \"a bad photo of an object\"\n",
    "], img_size=model.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classes_prompts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mevaluator\u001b[39;00m \u001b[39mimport\u001b[39;00m ContrastiveEvaluator\n\u001b[0;32m----> 3\u001b[0m class_evaluator \u001b[39m=\u001b[39m ContrastiveEvaluator(model, class_dataset, classes_prompts, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m)\n\u001b[1;32m      4\u001b[0m class_evaluator\u001b[39m.\u001b[39mevaluate()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classes_prompts' is not defined"
     ]
    }
   ],
   "source": [
    "from evaluator import ContrastiveEvaluator\n",
    "\n",
    "class_evaluator = ContrastiveEvaluator(model, class_dataset, classes_prompts, batch_size=64)\n",
    "class_evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "attr_dataset = PromptDataset(attr_samples_df, prompt_transform=lambda e: [\n",
    "    f\"a bad photo of a {e['attr_value']} object\", \n",
    "    \"a bad photo of an regular object\"\n",
    "], img_size=model.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.668%\n",
      "Precision: 0.567\n",
      "Recall: 0.564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 56.66811468288445,\n",
       " 'precision': 0.5670110593713621,\n",
       " 'recall': 0.5642195192586157}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluator import ContrastiveEvaluator\n",
    "\n",
    "attr_evaluator = ContrastiveEvaluator(model, attr_dataset, attr_prompts, batch_size=64)\n",
    "attr_evaluator.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "35ab78d29ed88d957c44548976856795b78553768444957779fbd79cbca61a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
