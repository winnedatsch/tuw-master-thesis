{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 571/571 [00:00<00:00, 335kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.34G/1.34G [00:12<00:00, 105MB/s] \n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 13.6kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.13MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.50MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The specified target token `next_to` does not exist in the model vocabulary. Replacing with `next`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.004163656383752823,\n",
       "  'token': 1998,\n",
       "  'token_str': 'and',\n",
       "  'sequence': 'a guitarist and a guitar'},\n",
       " {'score': 1.9909759885194944e-06,\n",
       "  'token': 2279,\n",
       "  'token_str': 'next',\n",
       "  'sequence': 'a guitarist next a guitar'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"a guitarist [MASK] a guitar\", targets=[\"and\", \"next_to\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../data/models/llama/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
      "llama_model_load_internal: mem required  = 14645.09 MB (+ 1026.00 MB per state)\n",
      ".\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama_init_from_file: kv self size  =  256.00 MB\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"../data/models/llama/7B/ggml-model-f16.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  4088.89 ms\n",
      "llama_print_timings:      sample time =     1.75 ms /     6 runs   (    0.29 ms per token)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\n",
      "llama_print_timings:        eval time =  1451.13 ms /     6 runs   (  241.85 ms per token)\n",
      "llama_print_timings:       total time =  1461.56 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-3f222a36-3efe-4988-862c-9849eddb59e7',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1686400352,\n",
       " 'model': '../data/models/llama/7B/ggml-model-f16.bin',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant', 'content': ' No.'},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 23, 'completion_tokens': 6, 'total_tokens': 29}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.create_chat_completion(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Does this phrase make sense: \\\"a car driving on a road\\\"?\"\n",
    "}], max_tokens=60, top_k=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhadl_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
