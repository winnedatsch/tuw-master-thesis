{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import reload_recursive\n",
    "\n",
    "# sys path hack to allow importing the encoding functions and other modules\n",
    "sys.path.insert(0, os.path.abspath(\"../src\"))\n",
    "sys.path.insert(0, os.path.abspath(\"../externals\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    gpu = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"Warning: no GPU detected, falling back to CPU\")\n",
    "    gpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean old model from cache\n",
    "if \"model\" in locals() or \"model\" in globals():\n",
    "    del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "%reload model.clip_model\n",
    "from model.clip_model import CLIPModel\n",
    "model = CLIPModel(gpu, model=\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"llm_model\" in locals() or \"llm_model\" in globals():\n",
    "    del llm_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "%reload model.opt_model\n",
    "from model.opt_model import OPTModel\n",
    "llm_model = OPTModel(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean old model from cache\n",
    "if \"object_detector\" in locals() or \"object_detector\" in globals():\n",
    "    del object_detector\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "%reload object_detection.owl_vit_object_detector\n",
    "from object_detection.owl_vit_object_detector import OWLViTObjectDetector\n",
    "object_detector = OWLViTObjectDetector(gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"../data/questions/val_sampled_questions_10000.json\") as f:\n",
    "   questions = list(json.load(f).items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clingo.control import Control \n",
    "%reload pipeline.encoding\n",
    "from pipeline.encoding import encode_question, encode_scene, sanitize_asp\n",
    "import time\n",
    "\n",
    "def answer_is_correct(answers, correct_answer):\n",
    "    correct = False \n",
    "\n",
    "    for answer in answers:\n",
    "        if answer == sanitize_asp(correct_answer): \n",
    "            correct = True\n",
    "        elif (answer == \"to_the_right_of\" and correct_answer == \"right\") or \\\n",
    "            (answer == \"to_the_left_of\" and correct_answer == \"left\") or \\\n",
    "            (answer == \"in_front_of\" and correct_answer == \"front\"):\n",
    "            correct = True\n",
    "    return correct \n",
    "\n",
    "def count_operators(question):\n",
    "    operations = [\"select\", \"query\", \"filter\", \"relate\", \"verify\", \"choose\", \"exist\", \"or\", \"different\", \"and\", \"same\", \"common\"]\n",
    "    op_counts = {f\"op_{op}\": 0 for op in operations}\n",
    "    for op in question[\"semantic\"]:\n",
    "        operator = op[\"operation\"].split(\" \")[0]\n",
    "        op_counts[f\"op_{operator}\"] += 1\n",
    "    return op_counts\n",
    "\n",
    "def is_scene_question(question):\n",
    "    return question[\"semantic\"][0][\"operation\"] == \"select\" and question[\"semantic\"][0][\"argument\"] == \"scene\"\n",
    "\n",
    "def evaluate_question(question, asp_theory): \n",
    "    op_counts = count_operators(question)\n",
    "    result = {\n",
    "        \"question_id\": question[\"qid\"], \n",
    "        \"semantic_str\": question[\"semanticStr\"], \n",
    "        \"image_id\": question[\"imageId\"],\n",
    "        \"answer\": question[\"answer\"],\n",
    "        **op_counts\n",
    "    }\n",
    "    \n",
    "    if is_scene_question(question):\n",
    "        return {**result, \"skipped\": True, \"model_response\": None, \"correct\": False, \"timeout\": False, \"runtime_sec\": 0.0}\n",
    "    else:\n",
    "        result[\"skipped\"] = False\n",
    "        start = time.time()\n",
    "        ctl = Control()\n",
    "        ctl.add(asp_theory)\n",
    "\n",
    "        scene_encoding = encode_scene(question, model, object_detector, llm_model)\n",
    "        question_encoding = encode_question(question)\n",
    "\n",
    "        with open(f\"../data/encoded_questions/{qid}.lp\", \"w\") as f:\n",
    "            f.write(\"% ------ scene encoding ------\\n\")\n",
    "            f.write(scene_encoding)\n",
    "            f.write(\"\\n% ------ question encoding ------\\n\")\n",
    "            f.write(question_encoding)\n",
    "\n",
    "        ctl.add(scene_encoding)\n",
    "        ctl.add(question_encoding)\n",
    "\n",
    "        answers = [[]]\n",
    "        def on_model(model):\n",
    "            answers[0] = [s.arguments[0].name for s in model.symbols(shown=True)]\n",
    "\n",
    "        ctl.ground()\n",
    "        handle = ctl.solve(on_model=on_model, async_ = True)\n",
    "        has_finished = handle.wait(timeout=10.0)\n",
    "        end = time.time()\n",
    "        result[\"timeout\"] = not has_finished\n",
    "        result[\"runtime_sec\"] = end - start\n",
    "\n",
    "        if len(answers[0]) > 0:\n",
    "            return {**result, \"model_response\": answers[0], \"correct\": answer_is_correct(answers[0], question[\"answer\"])}\n",
    "        else: \n",
    "            return {**result, \"model_response\": \"UNSAT\", \"correct\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b936554bc6cc43f7af94a6ea7570a47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Questions:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     200: Corr      90, Incorr     101, UNSAT       1, Skip     9, Corr %: 47.1204%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     48\u001b[0m question[\u001b[39m\"\u001b[39m\u001b[39mqid\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m qid\n\u001b[0;32m---> 49\u001b[0m results\u001b[39m.\u001b[39mappend(evaluate_question(question, theory))\n\u001b[1;32m     51\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m snapshot_steps \u001b[39m==\u001b[39m (snapshot_steps \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     52\u001b[0m     report_results(i)\n",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m, in \u001b[0;36mevaluate_question\u001b[0;34m(question, asp_theory)\u001b[0m\n\u001b[1;32m     44\u001b[0m ctl \u001b[39m=\u001b[39m Control()\n\u001b[1;32m     45\u001b[0m ctl\u001b[39m.\u001b[39madd(asp_theory)\n\u001b[0;32m---> 47\u001b[0m scene_encoding \u001b[39m=\u001b[39m encode_scene(question, model, object_detector, llm_model)\n\u001b[1;32m     48\u001b[0m question_encoding \u001b[39m=\u001b[39m encode_question(question)\n\u001b[1;32m     50\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../data/encoded_questions/\u001b[39m\u001b[39m{\u001b[39;00mqid\u001b[39m}\u001b[39;00m\u001b[39m.lp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/git/tuw-master-thesis/src/pipeline/encoding/scene_encoding.py:174\u001b[0m, in \u001b[0;36mencode_scene\u001b[0;34m(question, vlm_model, object_detector, llm_model)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[39mif\u001b[39;00m object2[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m rel_reference_main_perplexity_llm[object1[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]]:\n\u001b[1;32m    173\u001b[0m                 rel_samples \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mget_article(object1[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mobject1[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mrel\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mget_article(object2[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mobject2[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m rel \u001b[39min\u001b[39;00m sample_relations]\n\u001b[0;32m--> 174\u001b[0m                 rel_reference_main_perplexity_llm[object1[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]][object2[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m llm_model\u001b[39m.\u001b[39;49mscore(rel_samples)[\u001b[39m'\u001b[39m\u001b[39mmean_perplexity\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    176\u001b[0m \u001b[39m# add attributes derived from object detection (names, vposition/hposition)\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39mfor\u001b[39;00m o1, (oid1, object1) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(object_items):\n",
      "File \u001b[0;32m~/git/tuw-master-thesis/src/model/gpt_neo_model.py:14\u001b[0m, in \u001b[0;36mGPTNeoModel.score\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscore\u001b[39m(\u001b[39mself\u001b[39m, texts):\n\u001b[0;32m---> 14\u001b[0m     \u001b[39mreturn\u001b[39;00m perplexity(texts, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpu, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m)\n",
      "File \u001b[0;32m~/git/tuw-master-thesis/src/perplexity.py:67\u001b[0m, in \u001b[0;36mperplexity\u001b[0;34m(predictions, model, tokenizer, device, batch_size, add_start_token, max_length)\u001b[0m\n\u001b[1;32m     64\u001b[0m labels \u001b[39m=\u001b[39m encoded_batch\n\u001b[1;32m     66\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 67\u001b[0m     out_logits \u001b[39m=\u001b[39m model(encoded_batch, attention_mask\u001b[39m=\u001b[39;49mattn_mask)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     69\u001b[0m shift_logits \u001b[39m=\u001b[39m out_logits[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m     70\u001b[0m shift_labels \u001b[39m=\u001b[39m labels[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:741\u001b[0m, in \u001b[0;36mGPTNeoForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    735\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    739\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 741\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    742\u001b[0m     input_ids,\n\u001b[1;32m    743\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    744\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    745\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    746\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    747\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    748\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    749\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    750\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    751\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    752\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    753\u001b[0m )\n\u001b[1;32m    754\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    756\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:621\u001b[0m, in \u001b[0;36mGPTNeoModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    613\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    614\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    615\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m         head_mask[i],\n\u001b[1;32m    619\u001b[0m     )\n\u001b[1;32m    620\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    622\u001b[0m         hidden_states,\n\u001b[1;32m    623\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    624\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    625\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    626\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    627\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    628\u001b[0m     )\n\u001b[1;32m    630\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:326\u001b[0m, in \u001b[0;36mGPTNeoBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    324\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    325\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 326\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    327\u001b[0m     hidden_states,\n\u001b[1;32m    328\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    329\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    330\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    331\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    332\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    333\u001b[0m )\n\u001b[1;32m    334\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    335\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:278\u001b[0m, in \u001b[0;36mGPTNeoAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    270\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    271\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    277\u001b[0m ):\n\u001b[0;32m--> 278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    279\u001b[0m         hidden_states,\n\u001b[1;32m    280\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    281\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    282\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    283\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    284\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    285\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:241\u001b[0m, in \u001b[0;36mGPTNeoSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     present \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    243\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    244\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_proj(attn_output)\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:194\u001b[0m, in \u001b[0;36mGPTNeoSelfAttention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    191\u001b[0m mask_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfinfo(attn_weights\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin\n\u001b[1;32m    192\u001b[0m \u001b[39m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m mask_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(mask_value, dtype\u001b[39m=\u001b[39;49mattn_weights\u001b[39m.\u001b[39;49mdtype)\u001b[39m.\u001b[39;49mto(attn_weights\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    195\u001b[0m attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(causal_mask, attn_weights, mask_value)\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39m# Apply the attention mask\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from tqdm.notebook import tqdm \n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "with open(\"../src/pipeline/encoding/theory.lp\") as theory_file:\n",
    "    theory = theory_file.read()\n",
    "\n",
    "num_questions = 1000\n",
    "results = []\n",
    "history = []\n",
    "last_step = -1\n",
    "snapshot_steps = 200\n",
    "snapshot_dir = \"evaluations/val_1000_11\"\n",
    "\n",
    "if not os.path.exists(snapshot_dir):\n",
    "    os.mkdir(snapshot_dir)\n",
    "\n",
    "if os.path.isfile(f\"{snapshot_dir}/history.json\"):\n",
    "    with open(f\"{snapshot_dir}/history.json\") as f:\n",
    "        history = json.load(f)\n",
    "        last_step = history[-1][\"step\"] - 1\n",
    "    results_pd = pd.read_pickle(f\"{snapshot_dir}/results_snapshot.pkl\")\n",
    "    results = results_pd.to_dict(\"records\")\n",
    "\n",
    "progress_bar = tqdm(total=num_questions, desc=\"Questions\")\n",
    "\n",
    "def report_results(i):\n",
    "    results_pd = pd.DataFrame(results)\n",
    "    results_pd.to_pickle(f\"{snapshot_dir}/results_snapshot.pkl\")\n",
    "    num_skipped = results_pd[results_pd[\"skipped\"]].shape[0]\n",
    "    num_correct = results_pd[results_pd[\"correct\"]].shape[0]\n",
    "    num_incorrect = results_pd[~results_pd[\"skipped\"] & ~results_pd[\"correct\"]].shape[0]\n",
    "    num_unsat =  results_pd[results_pd[\"model_response\"] == \"UNSAT\"].shape[0]\n",
    "\n",
    "    history.append({\"step\": i+1, \"correct\": num_correct, \"incorrect\": num_incorrect, \"unsat\": num_unsat, \"skipped\": num_skipped, \"correct_percentage\": num_correct/(num_incorrect+num_correct)*100})\n",
    "    with open(f\"{snapshot_dir}/history.json\", 'w') as f:\n",
    "        json.dump(history, f, indent=4)\n",
    "\n",
    "    progress_bar.write(f\"Step {i+1:7d}: Corr {num_correct:7d}, Incorr {num_incorrect:7d}, UNSAT {num_unsat:7d}, Skip {num_skipped:5d}, Corr %: {(num_correct/(num_incorrect+num_correct)*100):.4f}%\")\n",
    "\n",
    "for i, (qid, question) in enumerate(list(islice(questions, 0, num_questions))):\n",
    "    if i <= last_step:\n",
    "        progress_bar.update(1)\n",
    "        continue\n",
    "    \n",
    "    question[\"qid\"] = qid\n",
    "    results.append(evaluate_question(question, theory))\n",
    "\n",
    "    if i % snapshot_steps == (snapshot_steps - 1):\n",
    "        report_results(i)\n",
    "    progress_bar.update(1)\n",
    "\n",
    "report_results(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
