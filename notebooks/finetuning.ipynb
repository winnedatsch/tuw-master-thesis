{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# sys path hack to allow importing the encoding functions and other modules\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "sys.path.insert(0, os.path.abspath('../externals'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    gpu = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"Warning: no GPU detected, falling back to CPU\")\n",
    "    gpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPImageProcessor, CLIPTokenizer\n",
    "\n",
    "clip_version = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(clip_version).to(gpu)\n",
    "image_processor = CLIPImageProcessor.from_pretrained(clip_version)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(clip_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reload_recursive\n",
    "\n",
    "%reload prompt_dataset\n",
    "from prompt_dataset import PromptDataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_rel_samples = pd.read_pickle(\"../data/finetuning/train_rel_samples_all.pkl\")\n",
    "training_attr_samples = pd.read_pickle(\"../data/finetuning/train_attr_samples_all.pkl\")\n",
    "training_samples = pd.concat([training_rel_samples, training_attr_samples])\n",
    "\n",
    "def compute_prompt(sample):\n",
    "    if not pd.isna(sample[\"object_name\"]):\n",
    "        # sample is for an attribute\n",
    "        return f\"{sample['attr_value']} {sample['object_name']}\"\n",
    "    else:\n",
    "        # sample is for a relation\n",
    "        return f\"{sample['object0_name']} {sample['rel']} {sample['object1_name']}\"\n",
    "    \n",
    "training_set = PromptDataset(training_samples, prompt_transform=compute_prompt, img_size=224, mode=\"scale\")\n",
    "training_loader = DataLoader(training_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_rel_samples = pd.read_pickle(\"../data/finetuning/val_rel_samples_10k.pkl\")\n",
    "validation_attr_samples = pd.read_pickle(\"../data/finetuning/val_attr_samples_10k.pkl\")\n",
    "validation_samples = pd.concat([validation_rel_samples, validation_attr_samples])\n",
    "\n",
    "validation_set = PromptDataset(validation_samples, prompt_transform=compute_prompt, img_size=224, mode=\"scale\")\n",
    "validation_loader = DataLoader(validation_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(708469, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7398, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollator\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "class CLIPCollator:\n",
    "    def __call__(self, features) -> Dict[str, Any]:\n",
    "        images = [f[0] for f in features]\n",
    "        texts = [f[1] for f in features]\n",
    "\n",
    "        image_inputs = image_processor(images, return_tensors=\"pt\", do_resize=False, do_center_crop=False)\n",
    "        text_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        return {**image_inputs, **text_inputs, \"return_loss\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_snapshots/clip_finetune2\",\n",
    "    learning_rate=2e-7,\n",
    "    warmup_ratio=0.25,\n",
    "    report_to=\"tensorboard\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=2000,\n",
    "    remove_unused_columns=False,\n",
    "    save_steps=2000,\n",
    "    save_total_limit=5,\n",
    "    logging_steps=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38271' max='66420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38271/66420 1:55:24 < 1:24:53, 5.53 it/s, Epoch 1.73/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.126400</td>\n",
       "      <td>1.213217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.811500</td>\n",
       "      <td>0.929310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.650200</td>\n",
       "      <td>0.813850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>0.754143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.525200</td>\n",
       "      <td>0.714633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.503500</td>\n",
       "      <td>0.681672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.478300</td>\n",
       "      <td>0.655320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.442500</td>\n",
       "      <td>0.639201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.423100</td>\n",
       "      <td>0.623831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.404900</td>\n",
       "      <td>0.611341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.399100</td>\n",
       "      <td>0.607743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.353700</td>\n",
       "      <td>0.608796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>0.602626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.348700</td>\n",
       "      <td>0.604740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.330100</td>\n",
       "      <td>0.596873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.332500</td>\n",
       "      <td>0.590539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.320700</td>\n",
       "      <td>0.591507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.322600</td>\n",
       "      <td>0.584785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.323100</td>\n",
       "      <td>0.588207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m tb_callback\u001b[39m.\u001b[39mtb_writer \u001b[39m=\u001b[39m writer\n\u001b[1;32m     18\u001b[0m trainer\u001b[39m.\u001b[39madd_callback(tb_callback)\n\u001b[0;32m---> 19\u001b[0m result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1667\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/transformers/trainer.py:2001\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1998\u001b[0m \u001b[39mif\u001b[39;00m optimizer_was_run \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed:\n\u001b[1;32m   1999\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m-> 2001\u001b[0m model\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[1;32m   2002\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   2003\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m steps_skipped) \u001b[39m/\u001b[39m steps_in_epoch\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/torch/nn/modules/module.py:2351\u001b[0m, in \u001b[0;36mModule.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m   2349\u001b[0m \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mgrad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2350\u001b[0m     \u001b[39mif\u001b[39;00m set_to_none:\n\u001b[0;32m-> 2351\u001b[0m         p\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2352\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2353\u001b[0m         \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mgrad_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "layout = {\n",
    "    \"combined\": {\n",
    "        \"loss\": [\"Multiline\", [\"train/loss\", \"eval/loss\"]]\n",
    "    },\n",
    "}\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/clip_finetune2\")\n",
    "writer.add_custom_scalars(layout)\n",
    "\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=training_set, eval_dataset=validation_set, data_collator=CLIPCollator())\n",
    "\n",
    "tb_callback = trainer.pop_callback(TensorBoardCallback)\n",
    "tb_callback.tb_writer = writer\n",
    "trainer.add_callback(tb_callback)\n",
    "result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OWL-ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForZeroShotObjectDetection, AutoProcessor\n",
    "\n",
    "owl_model = AutoModelForZeroShotObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "owl_processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "\n",
    "class PromptDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, scene_graphs):\n",
    "        self.scene_graphs = scene_graphs.items()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scene_graphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.scene_graphs[idx]\n",
    "        image = read_image(f\"../data/images/{entry['image_id']}.jpg\", ImageReadMode.RGB)\n",
    "\n",
    "        # crop bounding box\n",
    "        y,x,h,w = get_scaled_bbox(entry, image.shape[1], image.shape[2])\n",
    "        image = crop(image, y, x, h, w)\n",
    "\n",
    "        if self.mode == \"pad\":\n",
    "            # resize and scale (maintain aspect ratio)\n",
    "            if entry[\"bbox_h\"] > entry[\"bbox_w\"]:\n",
    "                resize_dimensions = (self.img_size, 2*round((self.img_size*entry[\"bbox_w\"]/entry[\"bbox_h\"])/2)) \n",
    "            else:\n",
    "                resize_dimensions = (2*round((self.img_size*entry[\"bbox_h\"]/entry[\"bbox_w\"])/2), self.img_size)\n",
    "            image = resize(image, resize_dimensions, antialias=True)\n",
    "\n",
    "            # pad the image to square dimensions\n",
    "            image = pad(image, ((self.img_size - resize_dimensions[1])//2, (self.img_size - resize_dimensions[0])//2))\n",
    "\n",
    "        elif self.mode == \"scale\":\n",
    "            # resize and scale the image to the target dimensions\n",
    "            image = resize(image, (self.img_size, self.img_size), antialias=True)\n",
    "\n",
    "        else: \n",
    "            raise RuntimeError(\"Unsupported image processing mode!\")\n",
    "\n",
    "        return (image, self.prompt_transform(entry), entry['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"owl_finetune\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-7,\n",
    "    warmup_ratio=0.2,\n",
    "    save_total_limit=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    remove_unused_columns=False,\n",
    "    logging_first_step=True,\n",
    "    save_steps=2000,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
