{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# sys path hack to allow importing the encoding functions and other modules\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "sys.path.insert(0, os.path.abspath('../externals'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    gpu = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"Warning: no GPU detected, falling back to CPU\")\n",
    "    gpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/jhadl/.conda/envs/jhadl_tf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-28 19:58:18.342079: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-28 19:58:18.366558: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-28 19:58:18.777780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPImageProcessor, CLIPTokenizer\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(gpu)\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_dataset import PromptDataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_rel_samples = pd.read_pickle(\"../data/finetuning/train_rel_samples_all.pkl\")\n",
    "training_rel_set = PromptDataset(training_rel_samples, prompt_transform=lambda e: [\n",
    "    f\"{e['object0_name']} {e['rel']} {e['object1_name']}\"\n",
    "], img_size=224, mode=\"pad\")\n",
    "training_loader = DataLoader(training_rel_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_rel_samples = pd.read_pickle(\"../data/finetuning/val_rel_samples_50k.pkl\")\n",
    "validation_rel_set = PromptDataset(validation_rel_samples, prompt_transform=lambda e: [\n",
    "    f\"{e['object0_name']} {e['rel']} {e['object1_name']}\"\n",
    "], img_size=224, mode=\"pad\")\n",
    "validation_loader = DataLoader(validation_rel_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "def get_linear_schedule_with_warumup(optimizer, len_dataset, batch_size, num_epochs, warmup_percentage=0.2):\n",
    "    steps_per_epoch = len_dataset // batch_size\n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "    warmup_steps = total_steps * warmup_percentage\n",
    "    return transformers.get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps - warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def loss_fn(logits):\n",
    "    labels = torch.arange(logits.shape[0])\n",
    "    loss_images = cross_entropy(logits, labels)\n",
    "    loss_texts = cross_entropy(logits.t(), labels)\n",
    "    return (loss_images + loss_texts) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(batch):\n",
    "    images = batch[0].unbind(0)\n",
    "    texts = list(batch[1][0])\n",
    "\n",
    "    image_inputs = image_processor(images, return_tensors=\"pt\", do_resize=False, do_center_crop=False).to(gpu)\n",
    "    text_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(gpu)\n",
    "\n",
    "    logits = model(**image_inputs, **text_inputs)[\"logits_per_image\"]\n",
    "    logits_cpu = logits.to(\"cpu\")\n",
    "    del images, texts, image_inputs, text_inputs, logits\n",
    "    return logits_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, optimizer, scheduler, training_loader, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, batch in enumerate(training_loader):\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        logits = predict(batch)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(logits)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:\n",
    "            last_loss = running_loss / 200 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "        del logits, loss\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(validation_loader):\n",
    "    running_vloss = 0.0\n",
    "    for i, batch in enumerate(validation_loader):\n",
    "        logits = predict(batch)\n",
    "        vloss = loss_fn(logits)\n",
    "        running_vloss += vloss.item()\n",
    "        del logits, vloss\n",
    "\n",
    "    return running_vloss / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 200 loss: 1.2607923579216003\n",
      "  batch 400 loss: 1.253549875319004\n",
      "  batch 600 loss: 1.248550270795822\n",
      "  batch 800 loss: 1.2203076487779618\n",
      "  batch 1000 loss: 1.1222080099582672\n",
      "  batch 1200 loss: 1.074778725504875\n",
      "  batch 1400 loss: 1.0473621149361134\n",
      "  batch 1600 loss: 0.9823834873735905\n",
      "  batch 1800 loss: 0.9771840846538544\n",
      "  batch 2000 loss: 0.9560783676803112\n",
      "  batch 2200 loss: 0.9312538653612137\n",
      "  batch 2400 loss: 0.8800676545500755\n",
      "  batch 2600 loss: 0.8826087538897991\n",
      "  batch 2800 loss: 0.8576292327046394\n",
      "  batch 3000 loss: 0.8183759136497974\n",
      "  batch 3200 loss: 0.8408213220536709\n",
      "  batch 3400 loss: 0.7943902204930783\n",
      "  batch 3600 loss: 0.7578974530100823\n",
      "  batch 3800 loss: 0.734758093804121\n",
      "  batch 4000 loss: 0.7302561473101378\n",
      "  batch 4200 loss: 0.7191734200716019\n",
      "  batch 4400 loss: 0.6998930731415749\n",
      "  batch 4600 loss: 0.6993444043397904\n",
      "  batch 4800 loss: 0.675366016253829\n",
      "  batch 5000 loss: 0.6951696416735649\n",
      "  batch 5200 loss: 0.6752667605876923\n",
      "  batch 5400 loss: 0.6837796939909458\n",
      "  batch 5600 loss: 0.6484435920417309\n",
      "  batch 5800 loss: 0.6695810703188181\n",
      "  batch 6000 loss: 0.655091255903244\n",
      "  batch 6200 loss: 0.634601208716631\n",
      "  batch 6400 loss: 0.6340318016707898\n",
      "  batch 6600 loss: 0.6170202147215604\n",
      "  batch 6800 loss: 0.631162209957838\n",
      "  batch 7000 loss: 0.5828922526538372\n",
      "  batch 7200 loss: 0.5953383800014853\n",
      "  batch 7400 loss: 0.6024974907934666\n",
      "  batch 7600 loss: 0.5868797190487385\n",
      "  batch 7800 loss: 0.573201657757163\n",
      "  batch 8000 loss: 0.5802429234981537\n",
      "  batch 8200 loss: 0.5690319515019655\n",
      "  batch 8400 loss: 0.5551386997103691\n",
      "  batch 8600 loss: 0.5711738388240337\n",
      "  batch 8800 loss: 0.5436495789140463\n",
      "  batch 9000 loss: 0.5345542401075363\n",
      "  batch 9200 loss: 0.5355809960514307\n",
      "  batch 9400 loss: 0.542882507443428\n",
      "  batch 9600 loss: 0.5514681053906679\n",
      "  batch 9800 loss: 0.5220880920439959\n",
      "  batch 10000 loss: 0.5037107564508915\n",
      "  batch 10200 loss: 0.5298627912253141\n",
      "  batch 10400 loss: 0.5008806620538234\n",
      "  batch 10600 loss: 0.5203574155271053\n",
      "  batch 10800 loss: 0.49765270195901395\n",
      "  batch 11000 loss: 0.5075957222282886\n",
      "  batch 11200 loss: 0.5030366542190313\n",
      "  batch 11400 loss: 0.5070156846567988\n",
      "  batch 11600 loss: 0.5195302415639163\n",
      "  batch 11800 loss: 0.5069570963084697\n",
      "  batch 12000 loss: 0.48802297182381155\n",
      "  batch 12200 loss: 0.4553155178204179\n",
      "  batch 12400 loss: 0.48634924918413164\n",
      "  batch 12600 loss: 0.47558196268975733\n",
      "  batch 12800 loss: 0.495508381575346\n",
      "  batch 13000 loss: 0.48445558071136474\n",
      "  batch 13200 loss: 0.4605058475583792\n",
      "  batch 13400 loss: 0.47658921729773285\n",
      "  batch 13600 loss: 0.4726451309770346\n",
      "  batch 13800 loss: 0.469621964097023\n",
      "  batch 14000 loss: 0.4653208946250379\n",
      "LOSS train 0.4653208946250379 validation 0.5189161341562367\n",
      "New best model found!\n",
      "EPOCH 2:\n",
      "  batch 200 loss: 0.40306184329092504\n",
      "  batch 400 loss: 0.43653829898685215\n",
      "  batch 600 loss: 0.4262879463285208\n",
      "  batch 800 loss: 0.43623379960656167\n",
      "  batch 1000 loss: 0.42263358674943446\n",
      "  batch 1200 loss: 0.4186493710428476\n",
      "  batch 1400 loss: 0.4272092178463936\n",
      "  batch 1600 loss: 0.3891842824220657\n",
      "  batch 1800 loss: 0.42950284253805876\n",
      "  batch 2000 loss: 0.39787007551640274\n",
      "  batch 2200 loss: 0.3931356007233262\n",
      "  batch 2400 loss: 0.4048409298807383\n",
      "  batch 2600 loss: 0.3999985271692276\n",
      "  batch 2800 loss: 0.4127093482762575\n",
      "  batch 3000 loss: 0.39270117446780206\n",
      "  batch 3200 loss: 0.4238950561732054\n",
      "  batch 3400 loss: 0.3958783721923828\n",
      "  batch 3600 loss: 0.4129233570024371\n",
      "  batch 3800 loss: 0.40005819439888\n",
      "  batch 4000 loss: 0.3893538156151772\n",
      "  batch 4200 loss: 0.3932639919221401\n",
      "  batch 4400 loss: 0.3898154656589031\n",
      "  batch 4600 loss: 0.38864714190363886\n",
      "  batch 4800 loss: 0.40106721967458725\n",
      "  batch 5000 loss: 0.38895453937351704\n",
      "  batch 5200 loss: 0.38061333417892457\n",
      "  batch 5400 loss: 0.3677742648124695\n",
      "  batch 5600 loss: 0.4044232888147235\n",
      "  batch 5800 loss: 0.4039127146452665\n",
      "  batch 6000 loss: 0.3827931556478143\n",
      "  batch 6200 loss: 0.390127319470048\n",
      "  batch 6400 loss: 0.3813234908133745\n",
      "  batch 6600 loss: 0.39280485451221464\n",
      "  batch 6800 loss: 0.38789231933653356\n",
      "  batch 7000 loss: 0.3697405579686165\n",
      "  batch 7200 loss: 0.37761456102132795\n",
      "  batch 7400 loss: 0.3842411703616381\n",
      "  batch 7600 loss: 0.3851330903917551\n",
      "  batch 7800 loss: 0.37952736906707285\n",
      "  batch 8000 loss: 0.35044204954057934\n",
      "  batch 8200 loss: 0.35573326993733645\n",
      "  batch 8400 loss: 0.3778684537112713\n",
      "  batch 8600 loss: 0.3717920445278287\n",
      "  batch 8800 loss: 0.352948704585433\n",
      "  batch 9000 loss: 0.3565557235106826\n",
      "  batch 9200 loss: 0.3703981539607048\n",
      "  batch 9400 loss: 0.3714968431740999\n",
      "  batch 9600 loss: 0.3651104058325291\n",
      "  batch 9800 loss: 0.3577792409807444\n",
      "  batch 10000 loss: 0.34815952632576225\n",
      "  batch 10200 loss: 0.37096171364188196\n",
      "  batch 10400 loss: 0.3580614351853728\n",
      "  batch 10600 loss: 0.3468600318953395\n",
      "  batch 10800 loss: 0.3584456261247396\n",
      "  batch 11000 loss: 0.3587719913944602\n",
      "  batch 11200 loss: 0.34814564071595666\n",
      "  batch 11400 loss: 0.334275851584971\n",
      "  batch 11600 loss: 0.3537817834317684\n",
      "  batch 11800 loss: 0.36639358691871166\n",
      "  batch 12000 loss: 0.34798936530947683\n",
      "  batch 12200 loss: 0.3595859033986926\n",
      "  batch 12400 loss: 0.3458900449424982\n",
      "  batch 12600 loss: 0.36208445945754647\n",
      "  batch 12800 loss: 0.3462426280602813\n",
      "  batch 13000 loss: 0.3574972803518176\n",
      "  batch 13200 loss: 0.35092364076524973\n",
      "  batch 13400 loss: 0.3507877299934626\n",
      "  batch 13600 loss: 0.3329865410923958\n",
      "  batch 13800 loss: 0.3400250756368041\n",
      "  batch 14000 loss: 0.3387132757715881\n",
      "LOSS train 0.3387132757715881 validation 0.46639536248157487\n",
      "New best model found!\n",
      "EPOCH 3:\n",
      "  batch 200 loss: 0.30339671958237885\n",
      "  batch 400 loss: 0.29549957875162364\n",
      "  batch 600 loss: 0.2948451219126582\n",
      "  batch 800 loss: 0.2747673459444195\n",
      "  batch 1000 loss: 0.29927707977592943\n",
      "  batch 1200 loss: 0.2916578692942858\n",
      "  batch 1400 loss: 0.3026329116895795\n",
      "  batch 1600 loss: 0.3008823783323169\n",
      "  batch 1800 loss: 0.2905301637388766\n",
      "  batch 2000 loss: 0.2953192605823278\n",
      "  batch 2200 loss: 0.3047927266731858\n",
      "  batch 2400 loss: 0.29283971946686504\n",
      "  batch 2600 loss: 0.29507474213838575\n",
      "  batch 2800 loss: 0.2975641161575913\n",
      "  batch 3000 loss: 0.29836450051516294\n",
      "  batch 3200 loss: 0.28229627426713705\n",
      "  batch 3400 loss: 0.28026400903239845\n",
      "  batch 3600 loss: 0.31420440509915354\n",
      "  batch 3800 loss: 0.28427530771121384\n",
      "  batch 4000 loss: 0.29419850232079625\n",
      "  batch 4200 loss: 0.2952784389257431\n",
      "  batch 4400 loss: 0.28708851657807827\n",
      "  batch 4600 loss: 0.2889558381959796\n",
      "  batch 4800 loss: 0.3009118980914354\n",
      "  batch 5000 loss: 0.3021227848157287\n",
      "  batch 5200 loss: 0.2883296907320619\n",
      "  batch 5400 loss: 0.2849255583062768\n",
      "  batch 5600 loss: 0.27393882282078263\n",
      "  batch 5800 loss: 0.27469271490350367\n",
      "  batch 6000 loss: 0.2874552376382053\n",
      "  batch 6200 loss: 0.283802157714963\n",
      "  batch 6400 loss: 0.30222265526652337\n",
      "  batch 6600 loss: 0.284526112601161\n",
      "  batch 6800 loss: 0.26799762204289435\n",
      "  batch 7000 loss: 0.293033563029021\n",
      "  batch 7200 loss: 0.28008661363273857\n",
      "  batch 7400 loss: 0.2734164196439087\n",
      "  batch 7600 loss: 0.28238417122513054\n",
      "  batch 7800 loss: 0.2891457166709006\n",
      "  batch 8000 loss: 0.2915551841631532\n",
      "  batch 8200 loss: 0.2835137163475156\n",
      "  batch 8400 loss: 0.2759233507514\n",
      "  batch 8600 loss: 0.2856432873755693\n",
      "  batch 8800 loss: 0.29799177406355737\n",
      "  batch 9000 loss: 0.2693043779581785\n",
      "  batch 9200 loss: 0.27761240074411037\n",
      "  batch 9400 loss: 0.2793915344774723\n",
      "  batch 9600 loss: 0.266284608989954\n",
      "  batch 9800 loss: 0.2897621199116111\n",
      "  batch 10000 loss: 0.2846348175406456\n",
      "  batch 10200 loss: 0.2793682476039976\n",
      "  batch 10400 loss: 0.28772339642047884\n",
      "  batch 10600 loss: 0.27718776647001503\n",
      "  batch 10800 loss: 0.28561594160273673\n",
      "  batch 11000 loss: 0.2658443226851523\n",
      "  batch 11200 loss: 0.2662603237107396\n",
      "  batch 11400 loss: 0.2767374512180686\n",
      "  batch 11600 loss: 0.2720380323007703\n",
      "  batch 11800 loss: 0.2910032204538584\n",
      "  batch 12000 loss: 0.289167410209775\n",
      "  batch 12200 loss: 0.28291225858032704\n",
      "  batch 12400 loss: 0.287944804225117\n",
      "  batch 12600 loss: 0.2884544534049928\n",
      "  batch 12800 loss: 0.28509996152482925\n",
      "  batch 13000 loss: 0.26643325477838514\n",
      "  batch 13200 loss: 0.2723467993550003\n",
      "  batch 13400 loss: 0.27715215109288693\n",
      "  batch 13600 loss: 0.2622288939356804\n",
      "  batch 13800 loss: 0.27231139317154884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 14000 loss: 0.27273024601861834\n",
      "LOSS train 0.27273024601861834 validation 0.4794739084255775\n",
      "EPOCH 4:\n",
      "  batch 200 loss: 0.25151115051470696\n",
      "  batch 400 loss: 0.24852171598002315\n",
      "  batch 600 loss: 0.25081537118181585\n",
      "  batch 800 loss: 0.26017503429204225\n",
      "  batch 1000 loss: 0.2611316084302962\n",
      "  batch 1200 loss: 0.25259262027218937\n",
      "  batch 1400 loss: 0.2601969661936164\n",
      "  batch 1600 loss: 0.24145988695323467\n",
      "  batch 1800 loss: 0.24964060816913844\n",
      "  batch 2000 loss: 0.2521200533397496\n",
      "  batch 2200 loss: 0.24789788333699106\n",
      "  batch 2400 loss: 0.2508187795802951\n",
      "  batch 2600 loss: 0.23587181514129044\n",
      "  batch 2800 loss: 0.2411006654240191\n",
      "  batch 3000 loss: 0.25045114394277335\n",
      "  batch 3200 loss: 0.25605797324329616\n",
      "  batch 3400 loss: 0.24252273278310896\n",
      "  batch 3600 loss: 0.2557850718125701\n",
      "  batch 3800 loss: 0.23374844616279006\n",
      "  batch 4000 loss: 0.24238831143826245\n",
      "  batch 4200 loss: 0.24150524575263263\n",
      "  batch 4400 loss: 0.246237119352445\n",
      "  batch 4600 loss: 0.2561848810128868\n",
      "  batch 4800 loss: 0.2524103962443769\n",
      "  batch 5000 loss: 0.2524388633668423\n",
      "  batch 5200 loss: 0.24365435250103473\n",
      "  batch 5400 loss: 0.23846394319087266\n",
      "  batch 5600 loss: 0.24094478338956832\n",
      "  batch 5800 loss: 0.23829921264201404\n",
      "  batch 6000 loss: 0.24242573950439691\n",
      "  batch 6200 loss: 0.23303408533334732\n",
      "  batch 6400 loss: 0.2513049479667097\n",
      "  batch 6600 loss: 0.22401629177853466\n",
      "  batch 6800 loss: 0.23101080665364862\n",
      "  batch 7000 loss: 0.250909939520061\n",
      "  batch 7200 loss: 0.24034678917378188\n",
      "  batch 7400 loss: 0.23480645883828402\n",
      "  batch 7600 loss: 0.23813441664911805\n",
      "  batch 7800 loss: 0.23088219173252583\n",
      "  batch 8000 loss: 0.23098348384723066\n",
      "  batch 8200 loss: 0.22357451980002224\n",
      "  batch 8400 loss: 0.23994332553818823\n",
      "  batch 8600 loss: 0.23978641505353152\n",
      "  batch 8800 loss: 0.22017701361328362\n",
      "  batch 9000 loss: 0.2441611343063414\n",
      "  batch 9200 loss: 0.2528937140572816\n",
      "  batch 9400 loss: 0.24321185061708092\n",
      "  batch 9600 loss: 0.2508827886544168\n",
      "  batch 9800 loss: 0.23471276056952775\n",
      "  batch 10000 loss: 0.22905994666740298\n",
      "  batch 10200 loss: 0.236803535297513\n",
      "  batch 10400 loss: 0.24286537131294608\n",
      "  batch 10600 loss: 0.24084601033478975\n",
      "  batch 10800 loss: 0.24564913904294372\n",
      "  batch 11000 loss: 0.24965478098019958\n",
      "  batch 11200 loss: 0.23847243435680865\n",
      "  batch 11400 loss: 0.23237643180415035\n",
      "  batch 11600 loss: 0.22952170541509986\n",
      "  batch 11800 loss: 0.23846938198432327\n",
      "  batch 12000 loss: 0.2457244998589158\n",
      "  batch 12200 loss: 0.22926986584439873\n",
      "  batch 12400 loss: 0.2451562262699008\n",
      "  batch 12600 loss: 0.24408391415141523\n",
      "  batch 12800 loss: 0.2364947484433651\n",
      "  batch 13000 loss: 0.23917648246511816\n",
      "  batch 13200 loss: 0.23227931428700685\n",
      "  batch 13400 loss: 0.22948236105963588\n",
      "  batch 13600 loss: 0.2510608889721334\n",
      "  batch 13800 loss: 0.24403872217983008\n",
      "  batch 14000 loss: 0.251905316747725\n",
      "LOSS train 0.251905316747725 validation 0.4865344988919745\n",
      "EPOCH 5:\n",
      "  batch 200 loss: 0.22541062284260988\n",
      "  batch 400 loss: 0.2334996734187007\n",
      "  batch 600 loss: 0.23070029102265835\n",
      "  batch 800 loss: 0.2490282134898007\n",
      "  batch 1000 loss: 0.2266889825090766\n",
      "  batch 1200 loss: 0.23162743287160992\n",
      "  batch 1400 loss: 0.2314370733872056\n",
      "  batch 1600 loss: 0.23488736908882857\n",
      "  batch 1800 loss: 0.24062409846112132\n",
      "  batch 2000 loss: 0.2365784122981131\n",
      "  batch 2200 loss: 0.21329739617183804\n",
      "  batch 2400 loss: 0.22035584051162005\n",
      "  batch 2600 loss: 0.22013972996734082\n",
      "  batch 2800 loss: 0.24220230286940933\n",
      "  batch 3000 loss: 0.21981043258681893\n",
      "  batch 3200 loss: 0.23068199656903743\n",
      "  batch 3400 loss: 0.22862319493666292\n",
      "  batch 3600 loss: 0.2283662180788815\n",
      "  batch 3800 loss: 0.23686262018978596\n",
      "  batch 4000 loss: 0.24694530272856355\n",
      "  batch 4200 loss: 0.2360380788706243\n",
      "  batch 4400 loss: 0.20355804786086082\n",
      "  batch 4600 loss: 0.22530315512791277\n",
      "  batch 4800 loss: 0.21622926529496908\n",
      "  batch 5000 loss: 0.22275218242779374\n",
      "  batch 5200 loss: 0.23190072210505605\n",
      "  batch 5400 loss: 0.22714809068478645\n",
      "  batch 5600 loss: 0.2327132857963443\n",
      "  batch 5800 loss: 0.23664839329198004\n",
      "  batch 6000 loss: 0.2287981296144426\n",
      "  batch 6200 loss: 0.24488661905750633\n",
      "  batch 6400 loss: 0.23553992267698048\n",
      "  batch 6600 loss: 0.23425617381930353\n",
      "  batch 6800 loss: 0.23317674478515982\n",
      "  batch 7000 loss: 0.23276775689795615\n",
      "  batch 7200 loss: 0.23956551562994718\n",
      "  batch 7400 loss: 0.22041925796307624\n",
      "  batch 7600 loss: 0.22177559030242264\n",
      "  batch 7800 loss: 0.21979159710928797\n",
      "  batch 8000 loss: 0.23686996618285774\n",
      "  batch 8200 loss: 0.2511919963732362\n",
      "  batch 8400 loss: 0.21652092441916465\n",
      "  batch 8600 loss: 0.23045071482658386\n",
      "  batch 8800 loss: 0.21420257551595567\n",
      "  batch 9000 loss: 0.24249092543497683\n",
      "  batch 9200 loss: 0.2246357363462448\n",
      "  batch 9400 loss: 0.24227369617670774\n",
      "  batch 9600 loss: 0.24082310494035483\n",
      "  batch 9800 loss: 0.2383768392726779\n",
      "  batch 10000 loss: 0.22522175829857588\n",
      "  batch 10200 loss: 0.23523550951853395\n",
      "  batch 10400 loss: 0.23193004298955203\n",
      "  batch 10600 loss: 0.23792647743597628\n",
      "  batch 10800 loss: 0.23040243722498416\n",
      "  batch 11000 loss: 0.23084949968382718\n",
      "  batch 11200 loss: 0.2301658403687179\n",
      "  batch 11400 loss: 0.24001320533454418\n",
      "  batch 11600 loss: 0.24567563584074378\n",
      "  batch 11800 loss: 0.2449869900662452\n",
      "  batch 12000 loss: 0.23464257651939988\n",
      "  batch 12200 loss: 0.23637302761897444\n",
      "  batch 12400 loss: 0.22358432943001388\n",
      "  batch 12600 loss: 0.22055267326533795\n",
      "  batch 12800 loss: 0.21802791219204665\n",
      "  batch 13000 loss: 0.22824949318543075\n",
      "  batch 13200 loss: 0.23473935028538107\n",
      "  batch 13400 loss: 0.23245569566264748\n",
      "  batch 13600 loss: 0.22893836260773243\n",
      "  batch 13800 loss: 0.21065907794982194\n",
      "  batch 14000 loss: 0.22094018783420324\n",
      "LOSS train 0.22094018783420324 validation 0.4850010560362931\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/clip_finetune_{}'.format(timestamp))\n",
    "\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-7)\n",
    "scheduler = get_linear_schedule_with_warumup(optimizer, training_rel_set.__len__(), training_loader.batch_size, EPOCHS)\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "\n",
    "    # Train for one epoch\n",
    "    avg_loss = train_one_epoch(epoch, optimizer, scheduler, training_loader, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Validate\n",
    "    avg_vloss = validate(validation_loader)\n",
    "\n",
    "    print('LOSS train {} validation {}'.format(avg_loss, avg_vloss))\n",
    "    writer.add_scalars('Training vs. Validation Loss', {\n",
    "        'Training' : avg_loss, \n",
    "        'Validation' : avg_vloss \n",
    "    }, epoch + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        print('New best model found!')\n",
    "        best_vloss = avg_vloss\n",
    "        model.save_pretrained(f\"model_snapshots/model_{timestamp}_{epoch}_{best_vloss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
