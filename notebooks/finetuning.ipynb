{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# sys path hack to allow importing the encoding functions and other modules\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "sys.path.insert(0, os.path.abspath('../externals'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    gpu = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"Warning: no GPU detected, falling back to CPU\")\n",
    "    gpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/jhadl/.conda/envs/jhadl_tf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPImageProcessor, CLIPTokenizer\n",
    "\n",
    "clip_version = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(clip_version).to(gpu)\n",
    "image_processor = CLIPImageProcessor.from_pretrained(clip_version)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(clip_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reload_recursive\n",
    "\n",
    "%reload prompt_dataset\n",
    "from prompt_dataset import PromptDataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_rel_samples = pd.read_pickle(\"../data/finetuning/train_rel_samples_all.pkl\")\n",
    "training_attr_samples = pd.read_pickle(\"../data/finetuning/train_attr_samples_all.pkl\")\n",
    "training_samples = pd.concat([training_rel_samples, training_attr_samples])\n",
    "\n",
    "def compute_prompt(sample):\n",
    "    if not pd.isna(sample[\"object_name\"]):\n",
    "        # sample is for an attribute\n",
    "        return f\"{sample['attr_value']} {sample['object_name']}\"\n",
    "    else:\n",
    "        # sample is for a relation\n",
    "        return f\"{sample['object0_name']} {sample['rel']} {sample['object1_name']}\"\n",
    "    \n",
    "training_set = PromptDataset(training_samples, prompt_transform=compute_prompt, img_size=224, mode=\"scale\")\n",
    "training_loader = DataLoader(training_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_rel_samples = pd.read_pickle(\"../data/finetuning/val_rel_samples_10k.pkl\")\n",
    "validation_attr_samples = pd.read_pickle(\"../data/finetuning/val_attr_samples_10k.pkl\")\n",
    "validation_samples = pd.concat([validation_rel_samples, validation_attr_samples])\n",
    "\n",
    "validation_set = PromptDataset(validation_samples, prompt_transform=compute_prompt, img_size=224, mode=\"scale\")\n",
    "validation_loader = DataLoader(validation_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(708469, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7398, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollator\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "class CLIPCollator:\n",
    "    def __call__(self, features) -> Dict[str, Any]:\n",
    "        images = [f[0] for f in features]\n",
    "        texts = [f[1] for f in features]\n",
    "\n",
    "        image_inputs = image_processor(images, return_tensors=\"pt\", do_resize=False, do_center_crop=False)\n",
    "        text_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        return {**image_inputs, **text_inputs, \"return_loss\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_snapshots/clip_finetune2\",\n",
    "    learning_rate=1e-7,\n",
    "    warmup_ratio=0.2,\n",
    "    report_to=\"tensorboard\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=2000,\n",
    "    remove_unused_columns=False,\n",
    "    save_steps=2000,\n",
    "    save_total_limit=5,\n",
    "    logging_steps=500,\n",
    "    logging_first_step=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/jhadl/.conda/envs/jhadl_tf/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1288' max='66420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1288/66420 03:17 < 2:46:46, 6.51 it/s, Epoch 0.06/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m tb_callback\u001b[39m.\u001b[39mtb_writer \u001b[39m=\u001b[39m writer\n\u001b[1;32m     18\u001b[0m trainer\u001b[39m.\u001b[39madd_callback(tb_callback)\n\u001b[0;32m---> 19\u001b[0m result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1667\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/transformers/trainer.py:1899\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1896\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1899\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1900\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1901\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mCLIPCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m      7\u001b[0m images \u001b[39m=\u001b[39m [f[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features]\n\u001b[1;32m      8\u001b[0m texts \u001b[39m=\u001b[39m [f[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features]\n\u001b[0;32m---> 10\u001b[0m image_inputs \u001b[39m=\u001b[39m image_processor(images, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, do_resize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, do_center_crop\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     11\u001b[0m text_inputs \u001b[39m=\u001b[39m tokenizer(texts, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mimage_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtext_inputs, \u001b[39m\"\u001b[39m\u001b[39mreturn_loss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m}\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/transformers/image_processing_utils.py:458\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[1;32m    457\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(images, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/transformers/models/clip/image_processing_clip.py:332\u001b[0m, in \u001b[0;36mCLIPImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image\u001b[39m=\u001b[39mimage, scale\u001b[39m=\u001b[39mrescale_factor) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    331\u001b[0m \u001b[39mif\u001b[39;00m do_normalize:\n\u001b[0;32m--> 332\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize(image\u001b[39m=\u001b[39;49mimage, mean\u001b[39m=\u001b[39;49mimage_mean, std\u001b[39m=\u001b[39;49mimage_std) \u001b[39mfor\u001b[39;49;00m image \u001b[39min\u001b[39;49;00m images]\n\u001b[1;32m    334\u001b[0m images \u001b[39m=\u001b[39m [to_channel_dimension_format(image, data_format) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    336\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: images}\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/transformers/models/clip/image_processing_clip.py:332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    329\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image\u001b[39m=\u001b[39mimage, scale\u001b[39m=\u001b[39mrescale_factor) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    331\u001b[0m \u001b[39mif\u001b[39;00m do_normalize:\n\u001b[0;32m--> 332\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize(image\u001b[39m=\u001b[39;49mimage, mean\u001b[39m=\u001b[39;49mimage_mean, std\u001b[39m=\u001b[39;49mimage_std) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    334\u001b[0m images \u001b[39m=\u001b[39m [to_channel_dimension_format(image, data_format) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    336\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: images}\n",
      "File \u001b[0;32m~/.conda/envs/jhadl_tf/lib/python3.11/site-packages/transformers/models/clip/image_processing_clip.py:217\u001b[0m, in \u001b[0;36mCLIPImageProcessor.normalize\u001b[0;34m(self, image, mean, std, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize\u001b[39m(\n\u001b[1;32m    197\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    198\u001b[0m     image: np\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    203\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    204\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m    Normalize an image. image = (image - image_mean) / image_std.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39m            The channel dimension format of the image. If not provided, it will be the same as the input image.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     \u001b[39mreturn\u001b[39;00m normalize(image, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, data_format\u001b[39m=\u001b[39;49mdata_format, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "layout = {\n",
    "    \"combined\": {\n",
    "        \"loss\": [\"Multiline\", [\"train/loss\", \"eval/loss\"]]\n",
    "    },\n",
    "}\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/clip_finetune2\")\n",
    "writer.add_custom_scalars(layout)\n",
    "\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=training_set, eval_dataset=validation_set, data_collator=CLIPCollator())\n",
    "\n",
    "tb_callback = trainer.pop_callback(TensorBoardCallback)\n",
    "tb_callback.tb_writer = writer\n",
    "trainer.add_callback(tb_callback)\n",
    "result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OWL-ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForZeroShotObjectDetection, AutoProcessor\n",
    "\n",
    "owl_model = AutoModelForZeroShotObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "owl_processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "\n",
    "class PromptDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, scene_graphs):\n",
    "        self.scene_graphs = scene_graphs.items()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scene_graphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.scene_graphs[idx]\n",
    "        image = read_image(f\"../data/images/{entry['image_id']}.jpg\", ImageReadMode.RGB)\n",
    "\n",
    "        # crop bounding box\n",
    "        y,x,h,w = get_scaled_bbox(entry, image.shape[1], image.shape[2])\n",
    "        image = crop(image, y, x, h, w)\n",
    "\n",
    "        if self.mode == \"pad\":\n",
    "            # resize and scale (maintain aspect ratio)\n",
    "            if entry[\"bbox_h\"] > entry[\"bbox_w\"]:\n",
    "                resize_dimensions = (self.img_size, 2*round((self.img_size*entry[\"bbox_w\"]/entry[\"bbox_h\"])/2)) \n",
    "            else:\n",
    "                resize_dimensions = (2*round((self.img_size*entry[\"bbox_h\"]/entry[\"bbox_w\"])/2), self.img_size)\n",
    "            image = resize(image, resize_dimensions, antialias=True)\n",
    "\n",
    "            # pad the image to square dimensions\n",
    "            image = pad(image, ((self.img_size - resize_dimensions[1])//2, (self.img_size - resize_dimensions[0])//2))\n",
    "\n",
    "        elif self.mode == \"scale\":\n",
    "            # resize and scale the image to the target dimensions\n",
    "            image = resize(image, (self.img_size, self.img_size), antialias=True)\n",
    "\n",
    "        else: \n",
    "            raise RuntimeError(\"Unsupported image processing mode!\")\n",
    "\n",
    "        return (image, self.prompt_transform(entry), entry['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"owl_finetune\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-7,\n",
    "    warmup_ratio=0.2,\n",
    "    save_total_limit=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    remove_unused_columns=False,\n",
    "    logging_first_step=True,\n",
    "    save_steps=2000,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
